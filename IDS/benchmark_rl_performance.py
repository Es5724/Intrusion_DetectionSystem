# -*- coding: utf-8 -*-

"""
RL ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ í…ŒìŠ¤íŠ¸

RL ì—ì´ì „íŠ¸ì˜ ì‘ë‹µ ì‹œê°„, í•™ìŠµ ì†ë„, ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë“±ì„ ì¸¡ì •í•©ë‹ˆë‹¤.
"""

import sys
import os
import time
import numpy as np
from datetime import datetime
import psutil
import gc

# ëª¨ë“ˆ ê²½ë¡œ ì¶”ê°€
current_dir = os.path.dirname(os.path.abspath(__file__))
sys.path.append(os.path.join(current_dir, 'modules'))

print("=" * 80)
print("          RL ì—ì´ì „íŠ¸ ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ í…ŒìŠ¤íŠ¸")
print("=" * 80)
print()


def benchmark_state_extraction():
    """ìƒíƒœ ì¶”ì¶œ ì†ë„ ë²¤ì¹˜ë§ˆí¬"""
    print("[ 1/5 ] ìƒíƒœ ì¶”ì¶œ ì†ë„ ë²¤ì¹˜ë§ˆí¬")
    print("-" * 60)
    
    try:
        from modules.rl_state_extractor import get_state_extractor
        
        extractor = get_state_extractor()
        
        # í…ŒìŠ¤íŠ¸ íŒ¨í‚· ìƒì„±
        test_packets = []
        for i in range(1000):
            packet = {
                'source': f'192.168.1.{i % 255}:{50000 + i}',
                'destination': f'10.0.0.{i % 255}:{80}',
                'protocol': ['TCP', 'UDP', 'ICMP'][i % 3],
                'length': np.random.randint(64, 1500),
                'timestamp': time.time()
            }
            test_packets.append(packet)
        
        # ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰
        start_time = time.time()
        
        for packet in test_packets:
            context = {'threat_probability': np.random.rand()}
            state = extractor.extract_state(packet, context)
        
        end_time = time.time()
        elapsed = end_time - start_time
        
        # ê²°ê³¼ ì¶œë ¥
        throughput = len(test_packets) / elapsed
        avg_time_ms = (elapsed / len(test_packets)) * 1000
        
        print(f"  ì²˜ë¦¬ íŒ¨í‚·: {len(test_packets)}ê°œ")
        print(f"  ì†Œìš” ì‹œê°„: {elapsed:.3f}ì´ˆ")
        print(f"  ì²˜ë¦¬ëŸ‰: {throughput:.1f} íŒ¨í‚·/ì´ˆ")
        print(f"  í‰ê·  ì‹œê°„: {avg_time_ms:.3f}ms/íŒ¨í‚·")
        
        # ì„±ëŠ¥ í‰ê°€
        if avg_time_ms < 1.0:
            print(f"  âœ… ì„±ëŠ¥: ìš°ìˆ˜ (< 1ms)")
        elif avg_time_ms < 5.0:
            print(f"  âœ… ì„±ëŠ¥: ì–‘í˜¸ (< 5ms)")
        else:
            print(f"  âš ï¸  ì„±ëŠ¥: ê°œì„  í•„ìš” (>= 5ms)")
        
        return True
        
    except Exception as e:
        print(f"  âŒ ë²¤ì¹˜ë§ˆí¬ ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()
        return False


def benchmark_rl_inference():
    """RL ì¶”ë¡  ì†ë„ ë²¤ì¹˜ë§ˆí¬"""
    print("\n[ 2/5 ] RL ì¶”ë¡  ì†ë„ ë²¤ì¹˜ë§ˆí¬")
    print("-" * 60)
    
    try:
        from modules.conservative_rl_agent import ConservativeRLAgent
        
        # ì—ì´ì „íŠ¸ ìƒì„±
        agent = ConservativeRLAgent(
            state_size=10,
            action_size=6,
            mode="standard"
        )
        
        # í…ŒìŠ¤íŠ¸ ìƒíƒœ ìƒì„±
        test_states = [np.random.rand(10) for _ in range(1000)]
        
        # ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰
        start_time = time.time()
        
        for state in test_states:
            action = agent.act(state)
        
        end_time = time.time()
        elapsed = end_time - start_time
        
        # ê²°ê³¼ ì¶œë ¥
        throughput = len(test_states) / elapsed
        avg_time_ms = (elapsed / len(test_states)) * 1000
        
        print(f"  ì¶”ë¡  íšŸìˆ˜: {len(test_states)}íšŒ")
        print(f"  ì†Œìš” ì‹œê°„: {elapsed:.3f}ì´ˆ")
        print(f"  ì²˜ë¦¬ëŸ‰: {throughput:.1f} ì¶”ë¡ /ì´ˆ")
        print(f"  í‰ê·  ì‹œê°„: {avg_time_ms:.3f}ms/ì¶”ë¡ ")
        
        # ì„±ëŠ¥ í‰ê°€
        if avg_time_ms < 0.5:
            print(f"  âœ… ì„±ëŠ¥: ìš°ìˆ˜ (< 0.5ms) - ì‹¤ì‹œê°„ ì²˜ë¦¬ ê°€ëŠ¥")
        elif avg_time_ms < 2.0:
            print(f"  âœ… ì„±ëŠ¥: ì–‘í˜¸ (< 2ms)")
        else:
            print(f"  âš ï¸  ì„±ëŠ¥: ê°œì„  í•„ìš” (>= 2ms)")
        
        return True
        
    except Exception as e:
        print(f"  âŒ ë²¤ì¹˜ë§ˆí¬ ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()
        return False


def benchmark_online_learning():
    """ì˜¨ë¼ì¸ í•™ìŠµ ì†ë„ ë²¤ì¹˜ë§ˆí¬"""
    print("\n[ 3/5 ] ì˜¨ë¼ì¸ í•™ìŠµ ì†ë„ ë²¤ì¹˜ë§ˆí¬")
    print("-" * 60)
    
    try:
        from modules.conservative_rl_agent import ConservativeRLAgent
        from modules.online_rl_trainer import OnlineRLTrainer
        
        # ì—ì´ì „íŠ¸ ë° í•™ìŠµê¸° ìƒì„±
        agent = ConservativeRLAgent(
            state_size=10,
            action_size=6,
            mode="standard",
            use_prioritized_replay=True,
            buffer_capacity=1000
        )
        
        # ê²½í—˜ ë°ì´í„° ìƒì„±
        print("  ê²½í—˜ ë°ì´í„° ìƒì„± ì¤‘ (100ê°œ)...")
        for i in range(100):
            state = np.random.rand(10)
            action = np.random.randint(0, 6)
            reward = np.random.randn() * 10
            next_state = np.random.rand(10)
            done = False
            
            agent.remember(state, action, reward, next_state, done)
        
        # í•™ìŠµ ë²¤ì¹˜ë§ˆí¬
        print("  í•™ìŠµ ë²¤ì¹˜ë§ˆí¬ ì‹œì‘ (10íšŒ í•™ìŠµ)...")
        
        learning_times = []
        
        for i in range(10):
            start_time = time.time()
            loss = agent.train(batch_size=32)
            end_time = time.time()
            
            learning_times.append(end_time - start_time)
        
        # ê²°ê³¼ ì¶œë ¥
        avg_learning_time = np.mean(learning_times) * 1000  # ms
        std_learning_time = np.std(learning_times) * 1000
        
        print(f"  í•™ìŠµ íšŸìˆ˜: {len(learning_times)}íšŒ")
        print(f"  í‰ê·  í•™ìŠµ ì‹œê°„: {avg_learning_time:.2f}ms Â± {std_learning_time:.2f}ms")
        print(f"  ìµœì†Œ ì‹œê°„: {min(learning_times) * 1000:.2f}ms")
        print(f"  ìµœëŒ€ ì‹œê°„: {max(learning_times) * 1000:.2f}ms")
        
        # 10ì´ˆ ì£¼ê¸° í•™ìŠµ ê°€ëŠ¥ì„± í‰ê°€
        if avg_learning_time < 100:  # 100ms ë¯¸ë§Œ
            print(f"  âœ… ì„±ëŠ¥: ìš°ìˆ˜ - 10ì´ˆ ì£¼ê¸° í•™ìŠµ ê°€ëŠ¥ (ì—¬ìœ : {10000 - avg_learning_time:.0f}ms)")
        elif avg_learning_time < 500:
            print(f"  âœ… ì„±ëŠ¥: ì–‘í˜¸ - 10ì´ˆ ì£¼ê¸° í•™ìŠµ ê°€ëŠ¥")
        else:
            print(f"  âš ï¸  ì„±ëŠ¥: 10ì´ˆ ì£¼ê¸° í•™ìŠµ ì–´ë ¤ì›€")
        
        return True
        
    except Exception as e:
        print(f"  âŒ ë²¤ì¹˜ë§ˆí¬ ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()
        return False


def benchmark_memory_usage():
    """ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë²¤ì¹˜ë§ˆí¬"""
    print("\n[ 4/5 ] ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë²¤ì¹˜ë§ˆí¬")
    print("-" * 60)
    
    try:
        process = psutil.Process()
        
        # ì´ˆê¸° ë©”ëª¨ë¦¬
        gc.collect()
        initial_memory = process.memory_info().rss / (1024 * 1024)
        print(f"  ì´ˆê¸° ë©”ëª¨ë¦¬: {initial_memory:.1f} MB")
        
        # ëª¨ë“ˆ ë¡œë“œ
        print("  ëª¨ë“ˆ ë¡œë”© ì¤‘...")
        from modules.rl_state_extractor import get_state_extractor
        from modules.realtime_reward_calculator import get_reward_calculator
        from modules.conservative_rl_agent import ConservativeRLAgent
        from modules.online_rl_trainer import OnlineRLTrainer
        
        after_import_memory = process.memory_info().rss / (1024 * 1024)
        import_overhead = after_import_memory - initial_memory
        print(f"  ë¡œë“œ í›„ ë©”ëª¨ë¦¬: {after_import_memory:.1f} MB (+{import_overhead:.1f} MB)")
        
        # ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
        print("  ì¸ìŠ¤í„´ìŠ¤ ìƒì„± ì¤‘...")
        extractor = get_state_extractor()
        calculator = get_reward_calculator()
        agent = ConservativeRLAgent(state_size=10, action_size=6)
        trainer = OnlineRLTrainer(agent)
        
        after_init_memory = process.memory_info().rss / (1024 * 1024)
        init_overhead = after_init_memory - after_import_memory
        print(f"  ìƒì„± í›„ ë©”ëª¨ë¦¬: {after_init_memory:.1f} MB (+{init_overhead:.1f} MB)")
        
        # ì‘ì—… ë¶€í•˜ í…ŒìŠ¤íŠ¸ (1000ê°œ íŒ¨í‚· ì²˜ë¦¬)
        print("  ì‘ì—… ë¶€í•˜ í…ŒìŠ¤íŠ¸ (1000ê°œ íŒ¨í‚· ì²˜ë¦¬)...")
        for i in range(1000):
            packet = {
                'source': f'192.168.1.{i % 255}',
                'destination': f'10.0.0.{i % 255}:80',
                'protocol': 'TCP',
                'length': np.random.randint(64, 1500),
                'timestamp': time.time()
            }
            context = {'threat_probability': np.random.rand()}
            state = extractor.extract_state(packet, context)
            action = agent.act(state)
            
            # ê²½í—˜ ì¶”ê°€
            if i % 10 == 0:
                reward = np.random.randn()
                next_state = np.random.rand(10)
                agent.remember(state, action, reward, next_state, False)
        
        after_workload_memory = process.memory_info().rss / (1024 * 1024)
        workload_overhead = after_workload_memory - after_init_memory
        print(f"  ì‘ì—… í›„ ë©”ëª¨ë¦¬: {after_workload_memory:.1f} MB (+{workload_overhead:.1f} MB)")
        
        # ê°€ë¹„ì§€ ì»¬ë ‰ì…˜ í›„
        gc.collect()
        after_gc_memory = process.memory_info().rss / (1024 * 1024)
        gc_saved = after_workload_memory - after_gc_memory
        print(f"  GC í›„ ë©”ëª¨ë¦¬: {after_gc_memory:.1f} MB (-{gc_saved:.1f} MB)")
        
        # ì´ ë©”ëª¨ë¦¬ ì¦ê°€ëŸ‰
        total_increase = after_gc_memory - initial_memory
        print(f"\n  ì´ ë©”ëª¨ë¦¬ ì¦ê°€: {total_increase:.1f} MB")
        
        # í‰ê°€
        if total_increase < 50:
            print(f"  âœ… ë©”ëª¨ë¦¬ ì‚¬ìš©: ë§¤ìš° íš¨ìœ¨ì  (< 50MB)")
        elif total_increase < 100:
            print(f"  âœ… ë©”ëª¨ë¦¬ ì‚¬ìš©: íš¨ìœ¨ì  (< 100MB)")
        elif total_increase < 200:
            print(f"  âœ… ë©”ëª¨ë¦¬ ì‚¬ìš©: ì–‘í˜¸ (< 200MB)")
        else:
            print(f"  âš ï¸  ë©”ëª¨ë¦¬ ì‚¬ìš©: ë†’ìŒ (>= 200MB)")
        
        return True
        
    except Exception as e:
        print(f"  âŒ ë²¤ì¹˜ë§ˆí¬ ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()
        return False


def benchmark_end_to_end_pipeline():
    """ì—”ë“œíˆ¬ì—”ë“œ íŒŒì´í”„ë¼ì¸ ë²¤ì¹˜ë§ˆí¬"""
    print("\n[ 5/5 ] ì—”ë“œíˆ¬ì—”ë“œ íŒŒì´í”„ë¼ì¸ ë²¤ì¹˜ë§ˆí¬")
    print("-" * 60)
    
    try:
        from modules.conservative_rl_agent import ConservativeRLAgent
        from modules.rl_state_extractor import get_state_extractor
        from modules.realtime_reward_calculator import get_reward_calculator
        from modules.online_rl_trainer import get_rl_integrator, get_online_trainer
        
        # ì‹œìŠ¤í…œ êµ¬ì„±
        print("  ì‹œìŠ¤í…œ êµ¬ì„± ì¤‘...")
        agent = ConservativeRLAgent(state_size=10, action_size=6, mode="standard")
        state_extractor = get_state_extractor()
        reward_calculator = get_reward_calculator()
        online_trainer = get_online_trainer(agent, learning_interval=1)
        integrator = get_rl_integrator(agent, state_extractor, reward_calculator, online_trainer)
        
        # ì „ì²´ íŒŒì´í”„ë¼ì¸ ë²¤ì¹˜ë§ˆí¬
        print("  ì „ì²´ íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸ (100ê°œ íŒ¨í‚·)...")
        
        pipeline_times = []
        
        for i in range(100):
            packet = {
                'source': f'192.168.1.{i % 255}:{50000 + i}',
                'destination': f'10.0.0.{i % 255}:80',
                'protocol': 'TCP',
                'length': np.random.randint(64, 1500),
                'timestamp': time.time()
            }
            
            rf_probability = np.random.rand()
            
            start = time.time()
            
            # 1. ìƒíƒœ ì¶”ì¶œ
            context = {'threat_probability': rf_probability}
            state = state_extractor.extract_state(packet, context)
            
            # 2. RL ì•¡ì…˜ ì„ íƒ
            action = agent.act(state)
            
            # 3. ë³´ìƒ ê³„ì‚°
            reward, details = reward_calculator.calculate_reward(
                threat_probability=rf_probability,
                action_taken=action
            )
            
            end = time.time()
            pipeline_times.append(end - start)
        
        # ê²°ê³¼ ì¶œë ¥
        avg_pipeline_time = np.mean(pipeline_times) * 1000  # ms
        std_pipeline_time = np.std(pipeline_times) * 1000
        p95_time = np.percentile(pipeline_times, 95) * 1000
        p99_time = np.percentile(pipeline_times, 99) * 1000
        
        print(f"\n  íŒŒì´í”„ë¼ì¸ ì„±ëŠ¥:")
        print(f"    í‰ê·  ì‹œê°„: {avg_pipeline_time:.3f}ms Â± {std_pipeline_time:.3f}ms")
        print(f"    ìµœì†Œ ì‹œê°„: {min(pipeline_times) * 1000:.3f}ms")
        print(f"    ìµœëŒ€ ì‹œê°„: {max(pipeline_times) * 1000:.3f}ms")
        print(f"    P95: {p95_time:.3f}ms")
        print(f"    P99: {p99_time:.3f}ms")
        print(f"    ì²˜ë¦¬ëŸ‰: {1000 / avg_pipeline_time:.1f} íŒ¨í‚·/ì´ˆ")
        
        # ì‹¤ì‹œê°„ ì²˜ë¦¬ ê°€ëŠ¥ì„± í‰ê°€
        max_acceptable_latency = 10.0  # 10ms
        
        if p99_time < max_acceptable_latency:
            print(f"  âœ… ì‹¤ì‹œê°„ ì²˜ë¦¬: ê°€ëŠ¥ (P99 < {max_acceptable_latency}ms)")
        else:
            print(f"  âš ï¸  ì‹¤ì‹œê°„ ì²˜ë¦¬: ì§€ì—° ê°€ëŠ¥ì„± (P99 >= {max_acceptable_latency}ms)")
        
        # í†µê³„ ì¶œë ¥
        reward_stats = reward_calculator.get_statistics()
        print(f"\n  ë³´ìƒ í†µê³„:")
        print(f"    TP: {reward_stats['tp_count']}, TN: {reward_stats['tn_count']}")
        print(f"    FP: {reward_stats['fp_count']}, FN: {reward_stats['fn_count']}")
        print(f"    í‰ê·  ë³´ìƒ: {reward_stats['avg_reward']:.2f}")
        
        print("  âœ… ì—”ë“œíˆ¬ì—”ë“œ íŒŒì´í”„ë¼ì¸ ë²¤ì¹˜ë§ˆí¬ ì„±ê³µ!")
        return True
        
    except Exception as e:
        print(f"  âŒ ë²¤ì¹˜ë§ˆí¬ ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()
        return False


def run_all_benchmarks():
    """ì „ì²´ ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰"""
    print("\nì‹œìŠ¤í…œ ì •ë³´:")
    print(f"  Python: {sys.version.split()[0]}")
    print(f"  OS: {os.name}")
    
    try:
        import psutil
        print(f"  CPU ì½”ì–´: {psutil.cpu_count()}ê°œ")
        memory = psutil.virtual_memory()
        print(f"  ì´ ë©”ëª¨ë¦¬: {memory.total / (1024**3):.1f} GB")
        print(f"  ì‚¬ìš© ê°€ëŠ¥: {memory.available / (1024**3):.1f} GB")
    except:
        pass
    
    print("\n" + "=" * 80)
    print()
    
    results = []
    
    # ê° ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰
    results.append(("ìƒíƒœ ì¶”ì¶œ ì†ë„", benchmark_state_extraction()))
    results.append(("RL ì¶”ë¡  ì†ë„", benchmark_rl_inference()))
    results.append(("ì˜¨ë¼ì¸ í•™ìŠµ ì†ë„", benchmark_online_learning()))
    results.append(("ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰", benchmark_memory_usage()))
    results.append(("ì—”ë“œíˆ¬ì—”ë“œ íŒŒì´í”„ë¼ì¸", benchmark_end_to_end_pipeline()))
    
    # ê²°ê³¼ ìš”ì•½
    print("\n" + "=" * 80)
    print("                      ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼ ìš”ì•½")
    print("=" * 80)
    
    success_count = sum(1 for _, result in results if result)
    total_count = len(results)
    
    for name, result in results:
        status = "âœ… PASS" if result else "âŒ FAIL"
        print(f"  {status}  {name}")
    
    print()
    print(f"  ì„±ê³µ: {success_count}/{total_count} ({success_count/total_count*100:.1f}%)")
    
    if success_count == total_count:
        print("\n  ğŸ‰ ëª¨ë“  ë²¤ì¹˜ë§ˆí¬ í†µê³¼! ì‹œìŠ¤í…œì´ ì„±ëŠ¥ ìš”êµ¬ì‚¬í•­ì„ ì¶©ì¡±í•©ë‹ˆë‹¤.")
        return 0
    else:
        print(f"\n  âš ï¸  {total_count - success_count}ê°œ ë²¤ì¹˜ë§ˆí¬ ì‹¤íŒ¨ - ì„±ëŠ¥ ê°œì„  í•„ìš”")
        return 1


if __name__ == "__main__":
    exit_code = run_all_benchmarks()
    
    print("\n" + "=" * 80)
    print(f"í…ŒìŠ¤íŠ¸ ì™„ë£Œ ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("=" * 80)
    
    sys.exit(exit_code)

