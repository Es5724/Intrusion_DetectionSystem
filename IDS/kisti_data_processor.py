#!/usr/bin/env python3
"""
KISTI-IDS-2022 ë°ì´í„°ì…‹ ì „ìš© ë¶„ì„ê¸° ë° ì „ì²˜ë¦¬ê¸°
5GB ëŒ€ìš©ëŸ‰ ë°ì´í„° íš¨ìœ¨ì  ì²˜ë¦¬, RF í•™ìŠµìš© ë°ì´í„° ìƒì„±
"""

import pandas as pd
import numpy as np
import os
import time
import logging
from datetime import datetime
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
import hashlib
import warnings
warnings.filterwarnings('ignore')

class KISTIDataProcessor:
    """KISTI-IDS-2022 ë°ì´í„°ì…‹ ì „ìš© ì²˜ë¦¬ê¸°"""
    
    def __init__(self, data_path="../data_set/training_set.csv", output_dir="processed_data"):
        self.data_path = data_path
        self.output_dir = output_dir
        
        # KISTI ë°ì´í„° êµ¬ì¡° ì •ì˜
        self.kisti_columns = [
            'uid', 'sourceIP', 'destinationIP', 'sourcePort', 'destinationPort',
            'protocol', 'directionType', 'jumboPayloadFlag', 'packetSize',
            'detectName', 'attackType', 'detectStart', 'detectEnd', 'orgIDX',
            'eventCount', 'analyResult', 'payload'
        ]
        
        # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
        os.makedirs(output_dir, exist_ok=True)
        
        # ë¡œê¹… ì„¤ì •
        logging.basicConfig(level=logging.INFO)
        self.logger = logging.getLogger('KISTIProcessor')
        
        print("KISTI-IDS-2022 ë°ì´í„° í”„ë¡œì„¸ì„œ ì´ˆê¸°í™” ì™„ë£Œ")
    
    def analyze_data_structure(self, sample_size=10000):
        """KISTI ë°ì´í„° êµ¬ì¡° ë¶„ì„ (ìƒ˜í”Œë§ ê¸°ë°˜)"""
        print("=== KISTI-IDS-2022 ë°ì´í„° êµ¬ì¡° ë¶„ì„ ===")
        
        try:
            # íŒŒì¼ í¬ê¸° í™•ì¸
            file_size_bytes = os.path.getsize(self.data_path)
            file_size_gb = file_size_bytes / (1024**3)
            print(f"íŒŒì¼ í¬ê¸°: {file_size_gb:.2f}GB")
            
            # ì „ì²´ í–‰ ìˆ˜ ì¶”ì • (ì²« 1000í–‰ìœ¼ë¡œ ì¶”ì •)
            print("ì „ì²´ í–‰ ìˆ˜ ì¶”ì • ì¤‘...")
            # íƒ­ êµ¬ë¶„ìë¡œ ì‹œë„
            try:
                sample_df = pd.read_csv(self.data_path, nrows=1000, sep='\t')
                print("âœ… íƒ­ êµ¬ë¶„ìë¡œ íŒŒì‹± ì„±ê³µ")
            except:
                # ê³µë°± êµ¬ë¶„ìë¡œ ì‹œë„
                try:
                    sample_df = pd.read_csv(self.data_path, nrows=1000, sep=' ', skipinitialspace=True)
                    print("âœ… ê³µë°± êµ¬ë¶„ìë¡œ íŒŒì‹± ì„±ê³µ")
                except:
                    # ê¸°ë³¸ ì‰¼í‘œ êµ¬ë¶„ì
                    sample_df = pd.read_csv(self.data_path, nrows=1000)
                    print("âœ… ì‰¼í‘œ êµ¬ë¶„ìë¡œ íŒŒì‹±")
            avg_row_size = file_size_bytes / len(sample_df) * 1000
            estimated_total_rows = int(file_size_bytes / avg_row_size)
            print(f"ì¶”ì • ì „ì²´ í–‰ ìˆ˜: {estimated_total_rows:,}ê°œ")
            
            # í—¤ë” ë¶„ì„
            print(f"ì»¬ëŸ¼ ìˆ˜: {len(sample_df.columns)}ê°œ")
            print("ì»¬ëŸ¼ ëª©ë¡:")
            for i, col in enumerate(sample_df.columns, 1):
                print(f"  {i:2d}. {col}")
            
            # ìƒ˜í”Œ ë°ì´í„°ë¡œ ìƒì„¸ ë¶„ì„
            print(f"\nìƒ˜í”Œ ë°ì´í„° ë¶„ì„ ({sample_size}í–‰)...")
            analysis_df = pd.read_csv(self.data_path, nrows=sample_size)
            
            return self._analyze_sample_data(analysis_df, estimated_total_rows)
            
        except Exception as e:
            print(f"ë°ì´í„° ë¶„ì„ ì‹¤íŒ¨: {e}")
            return None
    
    def _analyze_sample_data(self, df, total_rows):
        """ìƒ˜í”Œ ë°ì´í„° ìƒì„¸ ë¶„ì„"""
        analysis_result = {
            'total_estimated_rows': total_rows,
            'sample_size': len(df),
            'columns': list(df.columns),
            'column_count': len(df.columns)
        }
        
        # 1. ë ˆì´ë¸” ë¶„ì„ (attackType, analyResult)
        print("\n=== ë ˆì´ë¸” ë¶„ì„ ===")
        
        if 'attackType' in df.columns:
            attack_dist = df['attackType'].value_counts()
            analysis_result['attack_type_distribution'] = attack_dist.to_dict()
            
            print("ê³µê²© ìœ í˜• ë¶„í¬:")
            for attack_type, count in attack_dist.head(10).items():
                percentage = (count / len(df)) * 100
                print(f"  {attack_type}: {count}ê°œ ({percentage:.2f}%)")
        
        if 'analyResult' in df.columns:
            result_dist = df['analyResult'].value_counts()
            analysis_result['analysis_result_distribution'] = result_dist.to_dict()
            
            print("\në¶„ì„ ê²°ê³¼ ë¶„í¬:")
            for result, count in result_dist.items():
                percentage = (count / len(df)) * 100
                print(f"  {result}: {count}ê°œ ({percentage:.2f}%)")
        
        # 2. ë°ì´í„° í’ˆì§ˆ ë¶„ì„
        print("\n=== ë°ì´í„° í’ˆì§ˆ ë¶„ì„ ===")
        
        # ê²°ì¸¡ê°’ í™•ì¸
        missing_counts = df.isnull().sum()
        missing_cols = missing_counts[missing_counts > 0]
        if len(missing_cols) > 0:
            print(f"ê²°ì¸¡ê°’ ìˆëŠ” ì»¬ëŸ¼: {len(missing_cols)}ê°œ")
            analysis_result['missing_values'] = missing_cols.to_dict()
        else:
            print("ê²°ì¸¡ê°’: ì—†ìŒ")
            analysis_result['missing_values'] = {}
        
        # ì¤‘ë³µ í™•ì¸
        duplicates = df.duplicated().sum()
        duplicate_percentage = (duplicates / len(df)) * 100
        print(f"ì¤‘ë³µ í–‰: {duplicates}ê°œ ({duplicate_percentage:.2f}%)")
        analysis_result['duplicates'] = duplicates
        
        # 3. ë„¤íŠ¸ì›Œí¬ íŠ¹ì„± ë¶„ì„
        print("\n=== ë„¤íŠ¸ì›Œí¬ íŠ¹ì„± ë¶„ì„ ===")
        
        if 'protocol' in df.columns:
            protocol_dist = df['protocol'].value_counts()
            print("í”„ë¡œí† ì½œ ë¶„í¬:")
            for protocol, count in protocol_dist.head(5).items():
                percentage = (count / len(df)) * 100
                print(f"  {protocol}: {count}ê°œ ({percentage:.2f}%)")
        
        if 'packetSize' in df.columns:
            packet_stats = df['packetSize'].describe()
            print(f"\níŒ¨í‚· í¬ê¸° í†µê³„:")
            print(f"  í‰ê· : {packet_stats['mean']:.1f} bytes")
            print(f"  ìµœëŒ€: {packet_stats['max']:.0f} bytes")
            print(f"  ìµœì†Œ: {packet_stats['min']:.0f} bytes")
        
        # 4. ì‹œê°„ ì •ë³´ ë¶„ì„
        if 'detectStart' in df.columns and 'detectEnd' in df.columns:
            print("\nì‹œê°„ ê¸°ë°˜ ë¶„ë¦¬ ê°€ëŠ¥ì„±: âœ…")
            analysis_result['time_based_split'] = True
        else:
            print("\nì‹œê°„ ê¸°ë°˜ ë¶„ë¦¬: âŒ (ì‹œê°„ ì»¬ëŸ¼ ì—†ìŒ)")
            analysis_result['time_based_split'] = False
        
        return analysis_result
    
    def create_rf_training_data(self, chunk_size=50000, max_samples=500000):
        """RF í•™ìŠµìš© ë°ì´í„° ìƒì„± (ë©”ëª¨ë¦¬ íš¨ìœ¨ì )"""
        print("=== KISTI â†’ RF í•™ìŠµ ë°ì´í„° ë³€í™˜ ===")
        print(f"ğŸ“‹ ì²˜ë¦¬ ê³„íš: ìµœëŒ€ {max_samples:,}ê°œ ìƒ˜í”Œ, ì²­í¬ í¬ê¸°: {chunk_size:,}ê°œ")
        print(f"ğŸ“ íŒŒì¼ í¬ê¸°: {os.path.getsize(self.data_path) / (1024**3):.2f}GB")
        
        try:
            processed_chunks = []
            total_processed = 0
            
            # ì˜ˆìƒ ì²­í¬ ìˆ˜ ê³„ì‚°
            file_size = os.path.getsize(self.data_path)
            estimated_total_rows = file_size // 100  # ëŒ€ëµì  ì¶”ì •
            max_chunks = min(max_samples // chunk_size, estimated_total_rows // chunk_size)
            
            print(f"ğŸ“Š ì˜ˆìƒ ì²˜ë¦¬: ìµœëŒ€ {max_chunks}ê°œ ì²­í¬")
            
            # ì²­í¬ ë‹¨ìœ„ë¡œ ë°ì´í„° ì²˜ë¦¬ (êµ¬ë¶„ì ìë™ ê°ì§€)
            try:
                chunk_iter = pd.read_csv(self.data_path, chunksize=chunk_size, sep='\t')
                print("ğŸ“„ íƒ­ êµ¬ë¶„ìë¡œ ì²­í¬ ì²˜ë¦¬")
            except:
                try:
                    chunk_iter = pd.read_csv(self.data_path, chunksize=chunk_size, sep=' ', skipinitialspace=True)
                    print("ğŸ“„ ê³µë°± êµ¬ë¶„ìë¡œ ì²­í¬ ì²˜ë¦¬")
                except:
                    chunk_iter = pd.read_csv(self.data_path, chunksize=chunk_size)
                    print("ğŸ“„ ì‰¼í‘œ êµ¬ë¶„ìë¡œ ì²­í¬ ì²˜ë¦¬")
            
            start_time = time.time()
            
            for i, chunk in enumerate(chunk_iter):
                if total_processed >= max_samples:
                    break
                
                # ì§„í–‰ë¥  ê³„ì‚°
                progress_percentage = (total_processed / max_samples) * 100
                elapsed_time = time.time() - start_time
                
                if i > 0:  # ì²« ë²ˆì§¸ ì²­í¬ ì´í›„
                    avg_time_per_chunk = elapsed_time / i
                    remaining_chunks = (max_samples - total_processed) / chunk_size
                    eta_seconds = remaining_chunks * avg_time_per_chunk
                    eta_minutes = eta_seconds / 60
                    
                    print(f"ğŸ“Š ì²­í¬ {i+1} ì²˜ë¦¬ ì¤‘... ({len(chunk)}í–‰)")
                    print(f"   ì§„í–‰ë¥ : {progress_percentage:.1f}% | ê²½ê³¼ì‹œê°„: {elapsed_time/60:.1f}ë¶„ | ì˜ˆìƒì™„ë£Œ: {eta_minutes:.1f}ë¶„ í›„")
                else:
                    print(f"ğŸ“Š ì²­í¬ {i+1} ì²˜ë¦¬ ì¤‘... ({len(chunk)}í–‰)")
                    print(f"   ì§„í–‰ë¥ : {progress_percentage:.1f}% | ì‹œì‘ ë‹¨ê³„")
                
                # KISTI â†’ RF í˜•íƒœë¡œ ë³€í™˜
                processed_chunk = self._convert_kisti_to_rf_format(chunk)
                
                if processed_chunk is not None and len(processed_chunk) > 0:
                    processed_chunks.append(processed_chunk)
                    total_processed += len(processed_chunk)
                    
                    # ì„±ê³µë¥  ê³„ì‚°
                    success_rate = (len(processed_chunk) / len(chunk)) * 100
                    print(f"   âœ… ë³€í™˜ ì™„ë£Œ: {len(processed_chunk)}í–‰ (ì„±ê³µë¥ : {success_rate:.1f}%) | ëˆ„ì : {total_processed:,}í–‰")
                else:
                    print(f"   âŒ ë³€í™˜ ì‹¤íŒ¨: ì²­í¬ {i+1}")
                
                # 10ì²­í¬ë§ˆë‹¤ ìƒì„¸ ì§„í–‰ ìƒí™©
                if (i + 1) % 10 == 0:
                    memory_usage = total_processed * 0.001  # ì¶”ì • ë©”ëª¨ë¦¬ MB
                    print(f"ğŸ”„ ì¤‘ê°„ ì²´í¬í¬ì¸íŠ¸: ì²­í¬ {i+1}ê°œ ì™„ë£Œ")
                    print(f"   ì²˜ë¦¬ ì†ë„: {total_processed/elapsed_time:.0f} í–‰/ì´ˆ")
                    print(f"   ë©”ëª¨ë¦¬ ì‚¬ìš© ì¶”ì •: {memory_usage:.1f}MB")
                    print(f"   ë‚¨ì€ ì‘ì—…: {max_samples - total_processed:,}í–‰")
                    print("   " + "="*50)
                
                # ë©”ëª¨ë¦¬ ê´€ë¦¬
                del chunk
                
                if len(processed_chunks) >= 10:  # 10ê°œ ì²­í¬ë§ˆë‹¤ ì¤‘ê°„ ì €ì¥
                    self._save_intermediate_results(processed_chunks, i)
                    processed_chunks = []
            
            # ìµœì¢… í†µí•©
            if processed_chunks:
                final_df = pd.concat(processed_chunks, ignore_index=True)
            else:
                # ì¤‘ê°„ ì €ì¥ëœ íŒŒì¼ë“¤ í†µí•©
                final_df = self._load_intermediate_results()
            
            print(f"ìµœì¢… ì²˜ë¦¬ ì™„ë£Œ: {len(final_df)}í–‰")
            
            return final_df
            
        except Exception as e:
            print(f"ë°ì´í„° ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return None
    
    def _convert_kisti_to_rf_format(self, chunk):
        """KISTI ë°ì´í„°ë¥¼ RF í•™ìŠµ í˜•íƒœë¡œ ë³€í™˜"""
        try:
            # 1. ê¸°ë³¸ ë„¤íŠ¸ì›Œí¬ íŠ¹ì„± ì¶”ì¶œ
            rf_features = pd.DataFrame()
            
            # ê¸°ë³¸ íŠ¹ì„±ë“¤ (ì•ˆì „í•œ ë³€í™˜)
            rf_features['source'] = chunk['sourceIP'].astype(str) if 'sourceIP' in chunk.columns else 'unknown'
            rf_features['destination'] = chunk['destinationIP'].astype(str) if 'destinationIP' in chunk.columns else 'unknown'
            
            # í¬íŠ¸ ë²ˆí˜¸ (ì•ˆì „í•œ ìˆ«ì ë³€í™˜)
            rf_features['source_port'] = pd.to_numeric(chunk['sourcePort'], errors='coerce').fillna(0) if 'sourcePort' in chunk.columns else 0
            rf_features['dest_port'] = pd.to_numeric(chunk['destinationPort'], errors='coerce').fillna(0) if 'destinationPort' in chunk.columns else 0
            
            rf_features['protocol'] = chunk['protocol'].astype(str) if 'protocol' in chunk.columns else 'unknown'
            rf_features['packet_size'] = pd.to_numeric(chunk['packetSize'], errors='coerce').fillna(0) if 'packetSize' in chunk.columns else 0
            rf_features['event_count'] = pd.to_numeric(chunk['eventCount'], errors='coerce').fillna(1) if 'eventCount' in chunk.columns else 1
            
            # 2. ë ˆì´ë¸” ìƒì„± (KISTI â†’ RF í˜•íƒœ)
            rf_features['is_malicious'] = self._create_is_malicious_label(chunk)
            rf_features['attack_type'] = self._create_attack_type_label(chunk)
            
            # 3. ì¶”ê°€ íŠ¹ì„± ìƒì„± (CIC ìŠ¤íƒ€ì¼)
            rf_features['flow_duration'] = self._calculate_flow_duration(chunk)
            rf_features['direction_type'] = chunk['directionType'] if 'directionType' in chunk.columns else 0
            rf_features['jumbo_flag'] = chunk['jumboPayloadFlag'] if 'jumboPayloadFlag' in chunk.columns else 0
            
            # 4. ë°ì´í„° í’ˆì§ˆ ê°œì„ 
            rf_features = self._improve_data_quality(rf_features)
            
            return rf_features
            
        except Exception as e:
            self.logger.error(f"ë°ì´í„° ë³€í™˜ ì‹¤íŒ¨: {e}")
            return None
    
    def _create_is_malicious_label(self, chunk):
        """KISTI ë°ì´í„°ì—ì„œ is_malicious ë ˆì´ë¸” ìƒì„±"""
        if 'analyResult' in chunk.columns:
            # analyResult=2ë¥¼ ê³µê²©ìœ¼ë¡œ í•´ì„ (KISTI ì‹¤ì œ ë¶„í¬ ê¸°ë°˜)
            result_values = pd.to_numeric(chunk['analyResult'], errors='coerce').fillna(0)
            return (result_values == 2).astype(int)  # 2 = ê³µê²© íƒì§€ë¨
        elif 'attackType' in chunk.columns:
            # attackType ê¸°ë°˜ (ë³´ì¡°)
            attack_values = pd.to_numeric(chunk['attackType'], errors='coerce').fillna(0)
            return (attack_values != 0).astype(int)  # 0 = Normal, 1+ = ê³µê²©
        else:
            # ê¸°ë³¸ê°’: ëª¨ë‘ ì •ìƒìœ¼ë¡œ ì²˜ë¦¬
            return pd.Series([0] * len(chunk))
    
    def _create_attack_type_label(self, chunk):
        """KISTI ë°ì´í„°ì—ì„œ attack_type ë ˆì´ë¸” ìƒì„±"""
        # analyResult=2ì¸ ê²½ìš° íŠ¹ì • ê³µê²©ìœ¼ë¡œ ë¶„ë¥˜
        if 'analyResult' in chunk.columns:
            result_values = pd.to_numeric(chunk['analyResult'], errors='coerce').fillna(0)
            
            # analyResult=2ì¸ ê²½ìš° detectNameìœ¼ë¡œ ê³µê²© ìœ í˜• ì¶”ì •
            if 'detectName' in chunk.columns:
                # detectName í•´ì‹œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ê³µê²© ìœ í˜• ì¶”ì •
                attack_types = []
                for i, result in enumerate(result_values):
                    if result == 2:
                        # ì‹¤ì œë¡œëŠ” detectName í•´ì‹œë¡œ ê³µê²© ìœ í˜• ê²°ì •
                        # í˜„ì¬ëŠ” ê°„ë‹¨íˆ 'detected_attack'ìœ¼ë¡œ ë¶„ë¥˜
                        attack_types.append('detected_attack')
                    else:
                        attack_types.append('normal')
                return pd.Series(attack_types)
            else:
                # detectNameì´ ì—†ìœ¼ë©´ analyResultë§Œìœ¼ë¡œ ë¶„ë¥˜
                return pd.Series(['detected_attack' if r == 2 else 'normal' for r in result_values])
        
        elif 'attackType' in chunk.columns:
            # attackType ê¸°ë°˜ (ë³´ì¡°)
            attack_code_mapping = {
                0: 'normal',                    # Normal
                1: 'dos',                       # DoS
                2: 'port_scan',                 # Port Scanning
                3: 'fuzzing',                   # Fuzzing
                4: 'malware',                   # Malware
                5: 'brute_force',               # Dictionary Attack
                6: 'web_attack',                # Web Hacking
                7: 'brute_force',               # Brute Force
                8: 'infiltration',              # Infiltration
                9: 'web_attack',                # XSS
                10: 'web_attack',               # SQL Injection
                11: 'exploit'                   # Exploit
            }
            
            attack_codes = pd.to_numeric(chunk['attackType'], errors='coerce').fillna(-1)
            mapped_types = attack_codes.map(attack_code_mapping).fillna('unknown')
            return mapped_types
        else:
            return pd.Series(['normal'] * len(chunk))
    
    def _calculate_flow_duration(self, chunk):
        """í”Œë¡œìš° ì§€ì† ì‹œê°„ ê³„ì‚°"""
        if 'detectStart' in chunk.columns and 'detectEnd' in chunk.columns:
            try:
                # ì•ˆì „í•œ ì‹œê°„ ë³€í™˜
                start_times = pd.to_datetime(chunk['detectStart'], errors='coerce')
                end_times = pd.to_datetime(chunk['detectEnd'], errors='coerce')
                
                # ìœ íš¨í•œ ì‹œê°„ ë°ì´í„°ë§Œ ê³„ì‚°
                duration = (end_times - start_times).dt.total_seconds()
                return duration.fillna(0)
            except Exception as e:
                self.logger.warning(f"ì‹œê°„ ê³„ì‚° ì‹¤íŒ¨: {e}")
                return pd.Series([0] * len(chunk))
        else:
            return pd.Series([0] * len(chunk))
    
    def _improve_data_quality(self, df):
        """ë°ì´í„° í’ˆì§ˆ ê°œì„ """
        # 1. ê²°ì¸¡ê°’ ì²˜ë¦¬
        numeric_cols = df.select_dtypes(include=[np.number]).columns
        for col in numeric_cols:
            df[col].fillna(df[col].median(), inplace=True)
        
        # ë¬¸ìì—´ ì»¬ëŸ¼ ê²°ì¸¡ê°’ ì²˜ë¦¬
        string_cols = df.select_dtypes(include=['object']).columns
        for col in string_cols:
            df[col].fillna('unknown', inplace=True)
        
        # 2. ë¬´í•œê°’ ì²˜ë¦¬ (íƒ€ì… ë³€í™˜ ì „)
        df.replace([np.inf, -np.inf], np.nan, inplace=True)
        
        # 3. ë°ì´í„° íƒ€ì… ìµœì í™” (ì•ˆì „í•œ ë³€í™˜)
        for col in numeric_cols:
            if col in ['source_port', 'dest_port']:
                # í¬íŠ¸ ë²ˆí˜¸: 0-65535 ë²”ìœ„ë¡œ í´ë¦¬í•‘ í›„ ë³€í™˜
                df[col] = df[col].fillna(0).clip(0, 65535).astype('uint16')
            elif col in ['packet_size', 'event_count']:
                # ì–‘ìˆ˜ ê°’: 0 ì´ìƒìœ¼ë¡œ í´ë¦¬í•‘
                df[col] = df[col].fillna(0).clip(0, None).astype('uint32')
            elif col in ['is_malicious', 'jumbo_flag']:
                # ë°”ì´ë„ˆë¦¬ ê°’: 0 ë˜ëŠ” 1
                df[col] = df[col].fillna(0).clip(0, 1).astype('uint8')
        
        # 3. ì´ìƒê°’ ì²˜ë¦¬
        if 'packet_size' in df.columns:
            # íŒ¨í‚· í¬ê¸° ì´ìƒê°’ í´ë¦¬í•‘ (ìµœëŒ€ 65535)
            df['packet_size'] = df['packet_size'].clip(0, 65535)
        
        return df
    
    def create_advanced_train_test_split(self, df, test_ratio=0.2, val_ratio=0.1):
        """ê°•í™”ëœ train/test ë¶„ë¦¬ (í˜¸ìŠ¤íŠ¸ ê²©ë¦¬ + ì„¸ì…˜ ë¶„ë¦¬ + ëˆ„ìˆ˜ ë°©ì§€)"""
        print("\n=== ê°•í™”ëœ Train/Test ë¶„ë¦¬ ===")
        
        # 1. í˜¸ìŠ¤íŠ¸ ê¸°ë°˜ ê·¸ë£¹í•‘
        print("1. í˜¸ìŠ¤íŠ¸ ê¸°ë°˜ ê·¸ë£¹í•‘ ì¤‘...")
        df_with_groups = self._create_host_groups(df)
        
        # 2. ì„¸ì…˜ ê¸°ë°˜ ê·¸ë£¹í•‘
        print("2. ì„¸ì…˜ ê¸°ë°˜ ê·¸ë£¹í•‘ ì¤‘...")
        df_with_sessions = self._create_session_groups(df_with_groups)
        
        # 3. íŠ¹ì§• ëˆ„ìˆ˜ ì ê²€ ë° ì œê±°
        print("3. íŠ¹ì§• ëˆ„ìˆ˜ ì ê²€ ì¤‘...")
        df_clean = self._remove_leaky_features(df_with_sessions)
        
        # 4. ê·¸ë£¹ ê¸°ë°˜ ë¶„ë¦¬ (í˜¸ìŠ¤íŠ¸/ì„¸ì…˜ ë‹¨ìœ„)
        print("4. ê·¸ë£¹ ê¸°ë°˜ ë°ì´í„° ë¶„ë¦¬ ì¤‘...")
        train_df, val_df, test_df = self._group_based_split(df_clean, test_ratio, val_ratio)
        
        # 5. ë¶„ë¦¬ í’ˆì§ˆ ê²€ì¦
        print("5. ë¶„ë¦¬ í’ˆì§ˆ ê²€ì¦ ì¤‘...")
        self._validate_split_quality(train_df, val_df, test_df)
        
        return train_df, val_df, test_df
    
    def _create_host_groups(self, df):
        """í˜¸ìŠ¤íŠ¸ ê¸°ë°˜ ê·¸ë£¹ ìƒì„±"""
        # sourceIPì™€ destinationIP ì¡°í•©ìœ¼ë¡œ í˜¸ìŠ¤íŠ¸ ê·¸ë£¹ ìƒì„±
        df['host_pair'] = df['source'].astype(str) + "_" + df['destination'].astype(str)
        
        # í˜¸ìŠ¤íŠ¸ ê·¸ë£¹ë³„ í†µê³„
        host_stats = df['host_pair'].value_counts()
        print(f"   ê³ ìœ  í˜¸ìŠ¤íŠ¸ ìŒ: {len(host_stats)}ê°œ")
        print(f"   í‰ê·  í”Œë¡œìš°/í˜¸ìŠ¤íŠ¸: {host_stats.mean():.1f}ê°œ")
        
        # ì£¼ìš” í˜¸ìŠ¤íŠ¸ ê·¸ë£¹ (ìƒìœ„ 10ê°œ)
        major_hosts = host_stats.head(10)
        print(f"   ì£¼ìš” í˜¸ìŠ¤íŠ¸ ê·¸ë£¹:")
        for host, count in major_hosts.items():
            percentage = (count / len(df)) * 100
            print(f"     {host}: {count}ê°œ ({percentage:.1f}%)")
        
        return df
    
    def _create_session_groups(self, df):
        """ì„¸ì…˜ ê¸°ë°˜ ê·¸ë£¹ ìƒì„±"""
        # 5-tuple ê¸°ë°˜ ì„¸ì…˜ ì‹ë³„
        if 'uid' in df.columns:
            # uidê°€ ìˆìœ¼ë©´ ê·¸ëŒ€ë¡œ ì‚¬ìš©
            df['session_id'] = df['uid']
        else:
            # 5-tuple ê¸°ë°˜ ì„¸ì…˜ ID ìƒì„±
            df['session_id'] = (
                df['source'].astype(str) + "_" +
                df['destination'].astype(str) + "_" +
                df['source_port'].astype(str) + "_" +
                df['dest_port'].astype(str) + "_" +
                df['protocol'].astype(str)
            )
        
        # ì„¸ì…˜ í†µê³„
        session_stats = df['session_id'].value_counts()
        print(f"   ê³ ìœ  ì„¸ì…˜: {len(session_stats)}ê°œ")
        print(f"   í‰ê·  ì´ë²¤íŠ¸/ì„¸ì…˜: {session_stats.mean():.1f}ê°œ")
        
        # ì¥ê¸° ì„¸ì…˜ ë¶„ì„ (ë°ì´í„° ëˆ„ìˆ˜ ìœ„í—˜)
        long_sessions = session_stats[session_stats > 100]
        if len(long_sessions) > 0:
            print(f"   âš ï¸ ì¥ê¸° ì„¸ì…˜ {len(long_sessions)}ê°œ ë°œê²¬ (100+ ì´ë²¤íŠ¸)")
            print(f"     ìµœëŒ€ ì„¸ì…˜ ê¸¸ì´: {session_stats.max()}ê°œ ì´ë²¤íŠ¸")
        
        return df
    
    def _remove_leaky_features(self, df):
        """íŠ¹ì§• ëˆ„ìˆ˜ ì ê²€ ë° ì œê±°"""
        print("   íŠ¹ì§• ëˆ„ìˆ˜ ë¶„ì„ ì¤‘...")
        
        leaky_features = []
        
        # 1. íƒ€ê²Ÿê³¼ ê°•ìƒê´€ íŠ¹ì§• ê²€ì‚¬
        if 'is_malicious' in df.columns:
            numeric_features = df.select_dtypes(include=[np.number]).columns
            
            for feature in numeric_features:
                if feature != 'is_malicious':
                    try:
                        correlation = df[feature].corr(df['is_malicious'])
                        if abs(correlation) > 0.95:  # 95% ì´ìƒ ìƒê´€ê´€ê³„
                            leaky_features.append(f"{feature} (ìƒê´€ê³„ìˆ˜: {correlation:.3f})")
                    except:
                        pass
        
        # 2. ì‹œê°„ ëˆ„ìˆ˜ íŠ¹ì§• ê²€ì‚¬
        time_risk_features = ['detectStart', 'detectEnd', 'orgIDX']
        for feature in time_risk_features:
            if feature in df.columns:
                leaky_features.append(f"{feature} (ì‹œê°„ ì •ë³´ ëˆ„ìˆ˜)")
        
        # 3. ID ì„±ê²© íŠ¹ì§• ê²€ì‚¬
        id_risk_features = ['uid']
        for feature in id_risk_features:
            if feature in df.columns:
                # uidê°€ ìˆœì°¨ì ì´ê±°ë‚˜ íŒ¨í„´ì´ ìˆìœ¼ë©´ ìœ„í—˜
                if df[feature].dtype in ['object', 'int64']:
                    leaky_features.append(f"{feature} (ID ì •ë³´ ëˆ„ìˆ˜)")
        
        # 4. ê³µê²©ë³„ ê³ ìœ  íŠ¹ì§• ê²€ì‚¬
        if 'attack_type' in df.columns and 'detectName' in df.columns:
            # detectNameì´ attack_typeê³¼ 1:1 ë§¤í•‘ë˜ë©´ ëˆ„ìˆ˜
            detect_attack_mapping = df.groupby('detectName')['attack_type'].nunique()
            single_mapping = detect_attack_mapping[detect_attack_mapping == 1]
            if len(single_mapping) > 0:
                leaky_features.append(f"detectName (ê³µê²© ìœ í˜• ì§ì ‘ ë§¤í•‘)")
        
        # ëˆ„ìˆ˜ íŠ¹ì§• ì œê±°
        features_to_remove = []
        for leaky_desc in leaky_features:
            feature_name = leaky_desc.split(' ')[0]
            if feature_name in df.columns:
                features_to_remove.append(feature_name)
        
        if features_to_remove:
            print(f"   âš ï¸ ëˆ„ìˆ˜ ìœ„í—˜ íŠ¹ì§• ì œê±°: {len(features_to_remove)}ê°œ")
            for feature in features_to_remove:
                print(f"     - {feature}")
            
            df_clean = df.drop(columns=features_to_remove)
        else:
            print("   âœ… ëˆ„ìˆ˜ ìœ„í—˜ íŠ¹ì§• ì—†ìŒ")
            df_clean = df
        
        return df_clean
    
    def _group_based_split(self, df, test_ratio, val_ratio):
        """ê·¸ë£¹ ê¸°ë°˜ ë°ì´í„° ë¶„ë¦¬ (í˜¸ìŠ¤íŠ¸/ì„¸ì…˜ ë‹¨ìœ„)"""
        # í˜¸ìŠ¤íŠ¸ ê·¸ë£¹ë³„ ë¶„ë¦¬
        if 'host_pair' in df.columns:
            print("   í˜¸ìŠ¤íŠ¸ ê·¸ë£¹ ê¸°ë°˜ ë¶„ë¦¬ ì ìš©")
            return self._split_by_host_groups(df, test_ratio, val_ratio)
        
        # ì„¸ì…˜ë³„ ë¶„ë¦¬
        elif 'session_id' in df.columns:
            print("   ì„¸ì…˜ ê¸°ë°˜ ë¶„ë¦¬ ì ìš©")
            return self._split_by_sessions(df, test_ratio, val_ratio)
        
        # ì‹œê°„ ê¸°ë°˜ ë¶„ë¦¬ (ê¸°ë³¸)
        else:
            print("   ì‹œê°„ ê¸°ë°˜ ë¶„ë¦¬ ì ìš©")
            return self._split_by_time(df, test_ratio, val_ratio)
    
    def _split_by_host_groups(self, df, test_ratio, val_ratio):
        """í˜¸ìŠ¤íŠ¸ ê·¸ë£¹ë³„ ì™„ì „ ë¶„ë¦¬"""
        # í˜¸ìŠ¤íŠ¸ ê·¸ë£¹ ëª©ë¡
        unique_hosts = df['host_pair'].unique()
        np.random.shuffle(unique_hosts)  # ëœë¤ ì„ê¸°
        
        # ê·¸ë£¹ë³„ ë¶„ë¦¬ ì§€ì  ê³„ì‚°
        n_hosts = len(unique_hosts)
        train_hosts_end = int(n_hosts * (1 - test_ratio - val_ratio))
        val_hosts_end = int(n_hosts * (1 - test_ratio))
        
        # í˜¸ìŠ¤íŠ¸ ê·¸ë£¹ ë¶„í• 
        train_hosts = unique_hosts[:train_hosts_end]
        val_hosts = unique_hosts[train_hosts_end:val_hosts_end]
        test_hosts = unique_hosts[val_hosts_end:]
        
        # ë°ì´í„° ë¶„ë¦¬
        train_df = df[df['host_pair'].isin(train_hosts)].copy()
        val_df = df[df['host_pair'].isin(val_hosts)].copy()
        test_df = df[df['host_pair'].isin(test_hosts)].copy()
        
        print(f"   í˜¸ìŠ¤íŠ¸ ê·¸ë£¹ ë¶„ë¦¬:")
        print(f"     Train í˜¸ìŠ¤íŠ¸: {len(train_hosts)}ê°œ")
        print(f"     Val í˜¸ìŠ¤íŠ¸: {len(val_hosts)}ê°œ")
        print(f"     Test í˜¸ìŠ¤íŠ¸: {len(test_hosts)}ê°œ")
        
        return train_df, val_df, test_df
    
    def _split_by_sessions(self, df, test_ratio, val_ratio):
        """ì„¸ì…˜ë³„ ì™„ì „ ë¶„ë¦¬"""
        # ì„¸ì…˜ ëª©ë¡
        unique_sessions = df['session_id'].unique()
        np.random.shuffle(unique_sessions)
        
        # ì„¸ì…˜ë³„ ë¶„ë¦¬ ì§€ì  ê³„ì‚°
        n_sessions = len(unique_sessions)
        train_sessions_end = int(n_sessions * (1 - test_ratio - val_ratio))
        val_sessions_end = int(n_sessions * (1 - test_ratio))
        
        # ì„¸ì…˜ ë¶„í• 
        train_sessions = unique_sessions[:train_sessions_end]
        val_sessions = unique_sessions[train_sessions_end:val_sessions_end]
        test_sessions = unique_sessions[val_sessions_end:]
        
        # ë°ì´í„° ë¶„ë¦¬
        train_df = df[df['session_id'].isin(train_sessions)].copy()
        val_df = df[df['session_id'].isin(val_sessions)].copy()
        test_df = df[df['session_id'].isin(test_sessions)].copy()
        
        print(f"   ì„¸ì…˜ ê¸°ë°˜ ë¶„ë¦¬:")
        print(f"     Train ì„¸ì…˜: {len(train_sessions)}ê°œ")
        print(f"     Val ì„¸ì…˜: {len(val_sessions)}ê°œ")
        print(f"     Test ì„¸ì…˜: {len(test_sessions)}ê°œ")
        
        return train_df, val_df, test_df
    
    def _split_by_time(self, df, test_ratio, val_ratio):
        """ì‹œê°„ ê¸°ë°˜ ë¶„ë¦¬ (ê¸°ë³¸)"""
        if 'detectStart' in df.columns:
            df_sorted = df.sort_values('detectStart').reset_index(drop=True)
        else:
            df_sorted = df.sample(frac=1, random_state=42).reset_index(drop=True)
        
        total_size = len(df_sorted)
        train_end = int(total_size * (1 - test_ratio - val_ratio))
        val_end = int(total_size * (1 - test_ratio))
        
        train_df = df_sorted.iloc[:train_end].copy()
        val_df = df_sorted.iloc[train_end:val_end].copy()
        test_df = df_sorted.iloc[val_end:].copy()
        
        return train_df, val_df, test_df
    
    def _validate_split_quality(self, train_df, val_df, test_df):
        """ë¶„ë¦¬ í’ˆì§ˆ ê²€ì¦"""
        print("   ë¶„ë¦¬ í’ˆì§ˆ ê²€ì¦:")
        
        # 1. í˜¸ìŠ¤íŠ¸ ì¤‘ë³µ ê²€ì‚¬
        if 'host_pair' in train_df.columns:
            train_hosts = set(train_df['host_pair'].unique())
            val_hosts = set(val_df['host_pair'].unique())
            test_hosts = set(test_df['host_pair'].unique())
            
            host_overlap_val = len(train_hosts & val_hosts)
            host_overlap_test = len(train_hosts & test_hosts)
            
            print(f"     í˜¸ìŠ¤íŠ¸ ì¤‘ë³µ: Train-Val {host_overlap_val}ê°œ, Train-Test {host_overlap_test}ê°œ")
            
            if host_overlap_val == 0 and host_overlap_test == 0:
                print("     âœ… í˜¸ìŠ¤íŠ¸ ì™„ì „ ë¶„ë¦¬ í™•ì¸")
            else:
                print("     âš ï¸ í˜¸ìŠ¤íŠ¸ ì¤‘ë³µ ë°œê²¬")
        
        # 2. ì„¸ì…˜ ì¤‘ë³µ ê²€ì‚¬
        if 'session_id' in train_df.columns:
            train_sessions = set(train_df['session_id'].unique())
            val_sessions = set(val_df['session_id'].unique())
            test_sessions = set(test_df['session_id'].unique())
            
            session_overlap_val = len(train_sessions & val_sessions)
            session_overlap_test = len(train_sessions & test_sessions)
            
            print(f"     ì„¸ì…˜ ì¤‘ë³µ: Train-Val {session_overlap_val}ê°œ, Train-Test {session_overlap_test}ê°œ")
            
            if session_overlap_val == 0 and session_overlap_test == 0:
                print("     âœ… ì„¸ì…˜ ì™„ì „ ë¶„ë¦¬ í™•ì¸")
            else:
                print("     âš ï¸ ì„¸ì…˜ ì¤‘ë³µ ë°œê²¬")
        
        # 3. ì‹œê°„ ìˆœì„œ ê²€ì¦
        if 'detectStart' in train_df.columns:
            train_max_time = train_df['detectStart'].max()
            val_min_time = val_df['detectStart'].min()
            test_min_time = test_df['detectStart'].min()
            
            if train_max_time <= val_min_time <= test_min_time:
                print("     âœ… ì‹œê°„ ìˆœì„œ ë³´ì¥ í™•ì¸")
            else:
                print("     âš ï¸ ì‹œê°„ ìˆœì„œ ì—­ì „ ë°œê²¬")
        
        # 4. í´ë˜ìŠ¤ ë¶„í¬ ê· í˜• í™•ì¸
        for name, subset in [('Train', train_df), ('Val', val_df), ('Test', test_df)]:
            if 'is_malicious' in subset.columns:
                attack_ratio = subset['is_malicious'].mean()
                print(f"     {name} ê³µê²© ë¹„ìœ¨: {attack_ratio:.3f}")
                
                if 0.05 <= attack_ratio <= 0.5:  # 5-50% ë²”ìœ„
                    print(f"       âœ… ì ì ˆí•œ í´ë˜ìŠ¤ ë¶„í¬")
                else:
                    print(f"       âš ï¸ ê·¹ë‹¨ì  í´ë˜ìŠ¤ ë¶„í¬")
        
        # ë¶„ë¦¬ ê²°ê³¼ ì¶œë ¥
        total_size = len(train_df) + len(val_df) + len(test_df)
        print(f"ë¶„ë¦¬ ê²°ê³¼:")
        print(f"  Train: {len(train_df):,}í–‰ ({len(train_df)/total_size*100:.1f}%)")
        print(f"  Validation: {len(val_df):,}í–‰ ({len(val_df)/total_size*100:.1f}%)")
        print(f"  Test: {len(test_df):,}í–‰ ({len(test_df)/total_size*100:.1f}%)")
        
        # ê° ì„¸íŠ¸ì˜ í´ë˜ìŠ¤ ë¶„í¬ í™•ì¸
        for name, subset in [('Train', train_df), ('Validation', val_df), ('Test', test_df)]:
            if 'is_malicious' in subset.columns:
                attack_ratio = subset['is_malicious'].mean()
                print(f"  {name} ê³µê²© ë¹„ìœ¨: {attack_ratio:.3f}")
                
                if attack_ratio > 0:
                    attack_types = subset[subset['is_malicious']==1]['attack_type'].value_counts()
                    if len(attack_types) > 0:
                        print(f"    ì£¼ìš” ê³µê²©: {dict(attack_types.head(3))}")
        
        return train_df, val_df, test_df
    
    def _save_intermediate_results(self, chunks, batch_num):
        """ì¤‘ê°„ ê²°ê³¼ ì €ì¥ (ë©”ëª¨ë¦¬ ê´€ë¦¬)"""
        if chunks:
            intermediate_df = pd.concat(chunks, ignore_index=True)
            filename = os.path.join(self.output_dir, f"kisti_intermediate_batch_{batch_num}.csv")
            intermediate_df.to_csv(filename, index=False)
            print(f"ì¤‘ê°„ ì €ì¥: {filename} ({len(intermediate_df)}í–‰)")
            del intermediate_df
    
    def _load_intermediate_results(self):
        """ì¤‘ê°„ ì €ì¥ëœ íŒŒì¼ë“¤ í†µí•©"""
        intermediate_files = [f for f in os.listdir(self.output_dir) if f.startswith('kisti_intermediate_batch_')]
        
        if not intermediate_files:
            return pd.DataFrame()
        
        print("ì¤‘ê°„ ì €ì¥ íŒŒì¼ë“¤ í†µí•© ì¤‘...")
        dfs = []
        for file in intermediate_files:
            file_path = os.path.join(self.output_dir, file)
            df = pd.read_csv(file_path)
            dfs.append(df)
            os.remove(file_path)  # ì‚¬ìš© í›„ ì‚­ì œ
        
        return pd.concat(dfs, ignore_index=True)
    
    def save_processed_data(self, train_df, val_df, test_df):
        """ì²˜ë¦¬ëœ ë°ì´í„° ì €ì¥"""
        print("\n=== KISTI ì „ì²˜ë¦¬ ë°ì´í„° ì €ì¥ ===")
        
        # ì €ì¥í•  ì»¬ëŸ¼ ì„ íƒ (RF í•™ìŠµì— í•„ìš”í•œ ê²ƒë§Œ)
        rf_columns = [
            'source', 'destination', 'source_port', 'dest_port', 'protocol',
            'packet_size', 'flow_duration', 'event_count', 'direction_type',
            'jumbo_flag', 'is_malicious', 'attack_type'
        ]
        
        # ì‹¤ì œ ì¡´ì¬í•˜ëŠ” ì»¬ëŸ¼ë§Œ ì„ íƒ
        available_columns = [col for col in rf_columns if col in train_df.columns]
        
        for dataset_name, dataset in [('train', train_df), ('val', val_df), ('test', test_df)]:
            # RF í•™ìŠµìš© ì»¬ëŸ¼ë§Œ ì €ì¥
            clean_dataset = dataset[available_columns]
            
            # ì €ì¥
            output_path = os.path.join(self.output_dir, f"kisti_ids_2022_{dataset_name}.csv")
            clean_dataset.to_csv(output_path, index=False)
            
            print(f"  {dataset_name.upper()}: {output_path} ({len(clean_dataset):,}í–‰)")
            
            # í´ë˜ìŠ¤ ë¶„í¬ ì €ì¥
            if 'is_malicious' in clean_dataset.columns:
                attack_ratio = clean_dataset['is_malicious'].mean()
                attack_count = clean_dataset['is_malicious'].sum()
                normal_count = len(clean_dataset) - attack_count
                
                class_info = {
                    'total_samples': int(len(clean_dataset)),
                    'normal_samples': int(normal_count),
                    'attack_samples': int(attack_count),
                    'attack_ratio': float(attack_ratio),
                    'attack_types': clean_dataset['attack_type'].value_counts().to_dict()
                }
                
                info_path = os.path.join(self.output_dir, f"kisti_ids_2022_{dataset_name}_info.json")
                import json
                with open(info_path, 'w') as f:
                    json.dump(class_info, f, indent=2)
        
        # ì „ì²´ ìš”ì•½ ì €ì¥
        summary = {
            'dataset_name': 'KISTI-IDS-2022',
            'processing_date': datetime.now().isoformat(),
            'total_samples': len(train_df) + len(val_df) + len(test_df),
            'train_samples': len(train_df),
            'val_samples': len(val_df),
            'test_samples': len(test_df),
            'features_count': len(available_columns) - 2,  # is_malicious, attack_type ì œì™¸
            'available_features': available_columns
        }
        
        summary_path = os.path.join(self.output_dir, "kisti_dataset_summary.json")
        import json
        with open(summary_path, 'w') as f:
            json.dump(summary, f, indent=2)
        
        print(f"  ë°ì´í„°ì…‹ ìš”ì•½: {summary_path}")

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("KISTI-IDS-2022 ë°ì´í„° ì²˜ë¦¬ ì‹œì‘")
    print("=" * 60)
    
    try:
        # í”„ë¡œì„¸ì„œ ì´ˆê¸°í™”
        processor = KISTIDataProcessor()
        
        # 1. ë°ì´í„° êµ¬ì¡° ë¶„ì„
        print("1ë‹¨ê³„: ë°ì´í„° êµ¬ì¡° ë¶„ì„")
        analysis_result = processor.analyze_data_structure(sample_size=10000)
        
        if analysis_result is None:
            print("ë°ì´í„° ë¶„ì„ ì‹¤íŒ¨")
            return
        
        # 2. RF í•™ìŠµìš© ë°ì´í„° ìƒì„± (ìƒ˜í”Œë§)
        print("\n2ë‹¨ê³„: RF í•™ìŠµìš© ë°ì´í„° ìƒì„±")
        print("ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•´ 10ë§Œ ìƒ˜í”Œë¡œ ì œí•œ (ì„±ê³µ í›„ 50ë§Œìœ¼ë¡œ í™•ì¥)")
        
        processed_df = processor.create_rf_training_data(
            chunk_size=10000,  # ì‘ì€ ì²­í¬ë¡œ ë©”ëª¨ë¦¬ ì ˆì•½
            max_samples=100000  # ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•´ 10ë§Œ ìƒ˜í”Œë¡œ ì¶•ì†Œ
        )
        
        if processed_df is None or len(processed_df) == 0:
            print("ë°ì´í„° ë³€í™˜ ì‹¤íŒ¨")
            return
        
        # 3. ê°•í™”ëœ Train/Test ë¶„ë¦¬ (í˜¸ìŠ¤íŠ¸/ì„¸ì…˜ ê²©ë¦¬ + ëˆ„ìˆ˜ ë°©ì§€)
        print("\n3ë‹¨ê³„: ê°•í™”ëœ Train/Test ë¶„ë¦¬")
        train_df, val_df, test_df = processor.create_advanced_train_test_split(processed_df)
        
        # 4. ì²˜ë¦¬ëœ ë°ì´í„° ì €ì¥
        print("\n4ë‹¨ê³„: ë°ì´í„° ì €ì¥")
        processor.save_processed_data(train_df, val_df, test_df)
        
        print("\n=== KISTI-IDS-2022 ì „ì²˜ë¦¬ ì™„ë£Œ ===")
        print("ë‹¤ìŒ ë‹¨ê³„: RF ëª¨ë¸ ì¬í•™ìŠµ")
        print("ìƒì„±ëœ íŒŒì¼:")
        print("  - processed_data/kisti_ids_2022_train.csv")
        print("  - processed_data/kisti_ids_2022_val.csv")
        print("  - processed_data/kisti_ids_2022_test.csv")
        print("  - processed_data/kisti_dataset_summary.json")
        
    except Exception as e:
        print(f"ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()
