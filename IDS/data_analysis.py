#!/usr/bin/env python3
"""
IPS ì‹œìŠ¤í…œ ë°ì´í„° ë¶„í¬ ë¶„ì„ ìŠ¤í¬ë¦½íŠ¸
í˜„ì¬ ë°ì´í„°ì˜ í´ë˜ìŠ¤ ë¶„í¬, í’ˆì§ˆ, íŠ¹ì„±ì„ ë¶„ì„í•©ë‹ˆë‹¤.
"""

import pandas as pd
import numpy as np
import os
import glob
from collections import Counter
import matplotlib.pyplot as plt
import seaborn as sns

def analyze_current_data():
    """í˜„ì¬ ë°ì´í„° ìƒíƒœ ë¶„ì„"""
    print("=== IPS ë°ì´í„° ë¶„í¬ ë¶„ì„ ì‹œì‘ ===")
    
    # 1. ì‚¬ìš© ê°€ëŠ¥í•œ ë°ì´í„° íŒŒì¼ ì°¾ê¸°
    data_files = []
    
    # CSV íŒŒì¼ë“¤ ì°¾ê¸°
    csv_files = glob.glob("captured_packets_*.csv")
    csv_files.extend(glob.glob("../captured_packets_*.csv"))
    
    # ì „ì²˜ë¦¬ ë°ì´í„° ì°¾ê¸°
    if os.path.exists("data_set/ì „ì²˜ë¦¬ë°ì´í„°1.csv"):
        data_files.append("data_set/ì „ì²˜ë¦¬ë°ì´í„°1.csv")
    
    data_files.extend(csv_files)
    
    if not data_files:
        print("ë¶„ì„í•  ë°ì´í„° íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    print(f"ë°œê²¬ëœ ë°ì´í„° íŒŒì¼: {len(data_files)}ê°œ")
    for file in data_files[:5]:  # ìµœëŒ€ 5ê°œë§Œ í‘œì‹œ
        print(f"  - {file}")
    
    # 2. ê° íŒŒì¼ ë¶„ì„
    all_results = {}
    
    for file_path in data_files[:3]:  # ìµœëŒ€ 3ê°œ íŒŒì¼ë§Œ ë¶„ì„
        try:
            print(f"\n--- {file_path} ë¶„ì„ ì¤‘ ---")
            df = pd.read_csv(file_path)
            
            result = analyze_single_file(df, file_path)
            all_results[file_path] = result
            
        except Exception as e:
            print(f"íŒŒì¼ {file_path} ë¶„ì„ ì‹¤íŒ¨: {e}")
    
    # 3. ì¢…í•© ë¶„ì„ ê²°ê³¼
    print("\n=== ì¢…í•© ë¶„ì„ ê²°ê³¼ ===")
    generate_summary_report(all_results)

def analyze_single_file(df, file_path):
    """ê°œë³„ íŒŒì¼ ë¶„ì„"""
    result = {}
    
    print(f"ë°ì´í„° í¬ê¸°: {df.shape}")
    print(f"ì»¬ëŸ¼: {list(df.columns)}")
    
    # ê¸°ë³¸ í†µê³„
    result['shape'] = df.shape
    result['columns'] = list(df.columns)
    result['missing_values'] = df.isnull().sum().to_dict()
    result['data_types'] = df.dtypes.to_dict()
    
    # í´ë˜ìŠ¤ ë¶„í¬ ë¶„ì„ (ì—¬ëŸ¬ ê°€ëŠ¥í•œ ë ˆì´ë¸” ì»¬ëŸ¼ í™•ì¸)
    label_columns = ['protocol_6', 'is_malicious', 'attack', 'label', 'class']
    result['class_distributions'] = {}
    
    for col in label_columns:
        if col in df.columns:
            distribution = df[col].value_counts().to_dict()
            result['class_distributions'][col] = distribution
            print(f"\n{col} í´ë˜ìŠ¤ ë¶„í¬:")
            for class_name, count in distribution.items():
                percentage = (count / len(df)) * 100
                print(f"  {class_name}: {count:,}ê°œ ({percentage:.2f}%)")
    
    # íŠ¹ì„± ë¶„ì„
    numeric_cols = df.select_dtypes(include=[np.number]).columns
    if len(numeric_cols) > 0:
        result['numeric_stats'] = df[numeric_cols].describe().to_dict()
        
        # ì´ìƒì¹˜ íƒì§€
        for col in numeric_cols:
            q1 = df[col].quantile(0.25)
            q3 = df[col].quantile(0.75)
            iqr = q3 - q1
            outliers = df[(df[col] < q1 - 1.5*iqr) | (df[col] > q3 + 1.5*iqr)]
            result[f'{col}_outliers'] = len(outliers)
            if len(outliers) > 0:
                print(f"{col} ì´ìƒì¹˜: {len(outliers)}ê°œ ({len(outliers)/len(df)*100:.2f}%)")
    
    # ì¤‘ë³µ ë°ì´í„° ë¶„ì„
    duplicates = df.duplicated()
    result['duplicates'] = duplicates.sum()
    if duplicates.sum() > 0:
        print(f"ì¤‘ë³µ í–‰: {duplicates.sum()}ê°œ ({duplicates.sum()/len(df)*100:.2f}%)")
    
    # 5-íŠœí”Œ ì¤‘ë³µ ë¶„ì„ (ê°€ëŠ¥í•œ ê²½ìš°)
    tuple_cols = ['source', 'destination', 'protocol']
    if all(col in df.columns for col in tuple_cols):
        tuple_duplicates = df[tuple_cols].duplicated()
        result['tuple_duplicates'] = tuple_duplicates.sum()
        print(f"5-íŠœí”Œ ì¤‘ë³µ: {tuple_duplicates.sum()}ê°œ")
    
    return result

def generate_summary_report(all_results):
    """ì¢…í•© ë¶„ì„ ë³´ê³ ì„œ ìƒì„±"""
    
    print("\nğŸ“Š ë°ì´í„° í’ˆì§ˆ í‰ê°€:")
    
    total_samples = 0
    total_duplicates = 0
    class_imbalance_issues = []
    
    for file_path, result in all_results.items():
        samples = result['shape'][0]
        total_samples += samples
        
        # ì¤‘ë³µ ë°ì´í„° ì§‘ê³„
        if 'duplicates' in result:
            total_duplicates += result['duplicates']
        
        # í´ë˜ìŠ¤ ë¶ˆê· í˜• ë¶„ì„
        for col, distribution in result.get('class_distributions', {}).items():
            if len(distribution) >= 2:
                counts = list(distribution.values())
                max_count = max(counts)
                min_count = min(counts)
                imbalance_ratio = max_count / min_count
                
                if imbalance_ratio > 10:  # 10:1 ì´ìƒ ë¶ˆê· í˜•
                    class_imbalance_issues.append({
                        'file': file_path,
                        'column': col,
                        'ratio': imbalance_ratio,
                        'distribution': distribution
                    })
    
    print(f"ì´ ë°ì´í„° ìƒ˜í”Œ: {total_samples:,}ê°œ")
    print(f"ì´ ì¤‘ë³µ ë°ì´í„°: {total_duplicates:,}ê°œ ({total_duplicates/total_samples*100:.2f}%)")
    
    if class_imbalance_issues:
        print(f"\nâš ï¸ í´ë˜ìŠ¤ ë¶ˆê· í˜• ë¬¸ì œ ë°œê²¬: {len(class_imbalance_issues)}ê°œ")
        for issue in class_imbalance_issues:
            print(f"  {issue['file']} - {issue['column']}: {issue['ratio']:.1f}:1 ë¶ˆê· í˜•")
            
    # ê¶Œì¥ì‚¬í•­
    print("\nğŸ“‹ ê¶Œì¥ ê°œì„ ì‚¬í•­:")
    
    if total_duplicates > total_samples * 0.05:  # 5% ì´ìƒ ì¤‘ë³µ
        print("  1. ì¤‘ë³µ ì œê±° ì‹œìŠ¤í…œ êµ¬í˜„ í•„ìš”")
    
    if class_imbalance_issues:
        print("  2. í´ë˜ìŠ¤ ë¶ˆê· í˜• ì²˜ë¦¬ í•„ìš”")
        print("     - ê°€ì¤‘ì¹˜ ì†ì‹¤ í•¨ìˆ˜ ì ìš©")
        print("     - ì–¸ë”ìƒ˜í”Œë§ ë˜ëŠ” SMOTE ê³ ë ¤")
    
    if total_samples < 10000:
        print("  3. ë°ì´í„° ìˆ˜ì§‘ í™•ëŒ€ í•„ìš”")
        print("     - ë‹¤ì–‘í•œ ê³µê²© ì‹œë‚˜ë¦¬ì˜¤ ì¶”ê°€")
        print("     - ì •ìƒ íŠ¸ë˜í”½ ë°ì´í„° ë³´ê°•")
    
    print("  4. Train/Test ë¶„ë¦¬ ì‹œ ì‹œê°„ ê¸°ë°˜ ë¶„ë¦¬ ê¶Œì¥")
    print("  5. ì„¸ì…˜ ê¸°ë°˜ íŠ¹ì„± ì¶”ê°€ ê³ ë ¤")

def check_data_leakage_risk():
    """ë°ì´í„° ëˆ„ìˆ˜ ìœ„í—˜ ìš”ì†Œ ë¶„ì„"""
    print("\nğŸ” ë°ì´í„° ëˆ„ìˆ˜ ìœ„í—˜ ë¶„ì„:")
    
    # ì‹œê°„ ì •ë³´ í™•ì¸
    time_columns = ['timestamp', 'time', 'datetime', 'created_at']
    
    # ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” ë°ì´í„° íŒŒì¼ì„ ì½ì–´ì„œ ë¶„ì„
    print("  ê²€ì‚¬ í•­ëª©:")
    print("  - ë™ì¼ ì„¸ì…˜/í”Œë¡œìš°ì˜ train/test ë¶„ë¦¬ ì—¬ë¶€")
    print("  - ì‹œê°„ ìˆœì„œ ê¸°ë°˜ ë¶„ë¦¬ ì ìš© ì—¬ë¶€") 
    print("  - ë¯¸ë˜ ì •ë³´ ëˆ„ìˆ˜ ê°€ëŠ¥ì„±")
    print("  - íƒ€ê²Ÿ ëˆ„ìˆ˜ í”¼ì²˜ ì¡´ì¬ ì—¬ë¶€")
    
    # ê¶Œì¥ì‚¬í•­
    print("\n  ê¶Œì¥ ë¶„ë¦¬ ì „ëµ:")
    print("  1. ì‹œê°„ ê¸°ë°˜ ë¶„ë¦¬: 70% (ê³¼ê±°) / 30% (ë¯¸ë˜)")
    print("  2. ì„¸ì…˜ ê¸°ë°˜ ë¶„ë¦¬: ë™ì¼ í”Œë¡œìš°ëŠ” ê°™ì€ ì„¸íŠ¸ì—")
    print("  3. ê³„ì¸µì  ë¶„ë¦¬: ê³µê²© ìœ í˜•ë³„ ê· ë“± ë¶„ë°°")

def analyze_feature_quality():
    """íŠ¹ì„± í’ˆì§ˆ ë¶„ì„"""
    print("\nğŸ”¬ íŠ¹ì„± í’ˆì§ˆ ë¶„ì„:")
    
    print("  í˜„ì¬ íŠ¹ì„± (ì¶”ì •):")
    print("  - source, destination: IP ì£¼ì†Œ")
    print("  - protocol: í”„ë¡œí† ì½œ ë²ˆí˜¸/ì´ë¦„")
    print("  - length: íŒ¨í‚· í¬ê¸°")
    print("  - ttl, flags: TCP/IP íŠ¹ì„±")
    
    print("\n  í’ˆì§ˆ ê°œì„  í•„ìš” ì˜ì—­:")
    print("  1. ìŠ¤ì¼€ì¼ë§ ì œê±° (íŠ¸ë¦¬ ê¸°ë°˜ ëª¨ë¸)")
    print("  2. ë²”ì£¼í˜• ë³€ìˆ˜ ì¸ì½”ë”© ìµœì í™”")
    print("  3. ì‹œê³„ì—´ íŠ¹ì„± ì¶”ê°€ (ì„¸ì…˜í™”)")
    print("  4. ì»¨í…ìŠ¤íŠ¸ íŠ¹ì„± ì¶”ê°€")
    
    print("\n  ëˆ„ìˆ˜ ìœ„í—˜ íŠ¹ì„±:")
    print("  - ë¯¸ë˜ ì •ë³´ í¬í•¨ íŠ¹ì„± í™•ì¸ í•„ìš”")
    print("  - íƒ€ê²Ÿê³¼ ê°•ìƒê´€ íŠ¹ì„± ê²€í†  í•„ìš”")
    print("  - ID ê¸°ë°˜ íŠ¹ì„± ì œê±° í•„ìš”")

if __name__ == "__main__":
    try:
        analyze_current_data()
        check_data_leakage_risk()
        analyze_feature_quality()
        
        print("\n=== ë¶„ì„ ì™„ë£Œ ===")
        print("ë‹¤ìŒ ë‹¨ê³„: IPS_REDESIGN_TODO.md ì°¸ì¡°")
        
    except Exception as e:
        print(f"ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {e}")
        import traceback
        traceback.print_exc()
