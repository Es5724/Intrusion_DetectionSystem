# κ°•ν™”ν•™μµ μ•κ³ λ¦¬μ¦ λ¶„μ„ λ° κ°μ„  λ°©ν–¥

> **λ¬Έμ„ λ²„μ „**: v1.0  
> **μ‘μ„±μΌ**: 2025-09-16  
> **λ‹΄λ‹Ήμ**: IPS κ°λ°ν€  
> **λ©μ **: ν„μ¬ ν”„λ΅μ νΈμ κ°•ν™”ν•™μµ μ‹μ¤ν… λ¶„μ„ λ° ν–¥ν›„ κ°μ„  λ°©ν–¥ μ μ‹

---

## λ©μ°¨

1. [ν„μ¬ κ°•ν™”ν•™μµ μ‹μ¤ν… ν„ν™©](#ν„μ¬-κ°•ν™”ν•™μµ-μ‹μ¤ν…-ν„ν™©)
2. [μ•κ³ λ¦¬μ¦λ³„ μƒμ„Έ λ¶„μ„](#μ•κ³ λ¦¬μ¦λ³„-μƒμ„Έ-λ¶„μ„)
3. [ν„μ¬ μ‚¬μ© κµ¬μ΅°](#ν„μ¬-μ‚¬μ©-κµ¬μ΅°)
4. [μ¥λ‹¨μ  λΉ„κµ λ¶„μ„](#μ¥λ‹¨μ -λΉ„κµ-λ¶„μ„)
5. [κ°μ„  λ°©ν–¥ λ° κ¶μ¥μ‚¬ν•­](#κ°μ„ -λ°©ν–¥-λ°-κ¶μ¥μ‚¬ν•­)
6. [κµ¬ν„ μ°μ„ μμ„](#κµ¬ν„-μ°μ„ μμ„)

---

## ν„μ¬ κ°•ν™”ν•™μµ μ‹μ¤ν… ν„ν™©

###  **μ‹¤μ  μ‚¬μ© μ¤‘μΈ μ•κ³ λ¦¬μ¦**

**λ©”μΈ μ•κ³ λ¦¬μ¦**: **Conservative Q-Learning** (λ‹¨λ… μ‚¬μ©) β­
- **νμΌ**: `IDS/modules/conservative_rl_agent.py`
- **ν™κ²½**: `IDS/modules/defense_policy_env.py`
- **ν‰κ°€**: `IDS/modules/ope_evaluator.py`

**λ°±μ—… μ•κ³ λ¦¬μ¦**: **Double DQN** (νΈν™μ„± μ μ§€μ©)
- **νμΌ**: `IDS/modules/reinforcement_learning.py`
- **μƒνƒ**: λΉ„ν™μ„±ν™” (μ½”λ“μƒ μ΅΄μ¬ν•μ§€λ§ μ‹¤μ  μ‚¬μ© μ•ν•¨)
- **μ©λ„**: κ°λ°/ν…μ¤νΈ, μ•κ³ λ¦¬μ¦ λΉ„κµ, Fallback
 **μ‹μ¤ν… μ•„ν‚¤ν…μ²**

```mermaid
flowchart TD
    A[KISTI RF νƒμ§€<br/>F1=0.95] --> B[μ„ν‘ μ •λ³΄ μƒμ„±<br/>ν™•λ¥ , μ‹ λΆ°λ„, κ³µκ²©μ ν•]
    B --> C[DefensePolicyEnv<br/>10μ°¨μ› μƒνƒ λ³€ν™]
    C --> D[ConservativeRLAgent<br/>6κ° μ•΅μ… μ„ νƒ]
    D --> E[λ°©μ–΄ μ΅°μΉ μ‹¤ν–‰<br/>IPμ°¨λ‹¨, λ μ΄νΈμ ν• λ“±]
    E --> F[OPE μ„±λ¥ ν‰κ°€<br/>IS, DR, WIS]
    F -.-> D
    
    G[κΈ°μ΅΄ DQN<br/>reinforcement_learning.py]
    G -.-> H[νΈν™μ„± μ μ§€<br/>λΉ„ν™μ„±ν™” μƒνƒ]
    
    style D fill:#ff6b6b,stroke:#d63031,stroke-width:3px,color:#fff
    style G fill:#ddd,stroke:#999,stroke-width:1px,color:#666
```

---

## μ•κ³ λ¦¬μ¦λ³„ μƒμ„Έ λ¶„μ„

### 1. Conservative Q-Learning (ν„μ¬ λ©”μΈ)

#### **ν•µμ‹¬ νΉμ§•**
```python
# Conservative μ†μ‹¤ ν•¨μ
td_loss = MSELoss(current_q_values, target_q_values)
conservative_penalty = logsumexp(all_q_values).mean() - current_q_values.mean()
total_loss = td_loss + alpha_cql * conservative_penalty
```

#### **νλΌλ―Έν„° μ„¤μ • (λ³΄μμ  μ ‘κ·Ό)**
| νλΌλ―Έν„° | κ°’ | λ©μ  |
|----------|----|----- |
| `alpha_cql` | 1.0 | Conservative μ •κ·ν™” κ³„μ |
| `gamma` | 0.99 | λ†’μ€ ν• μΈμ¨ (μ¥κΈ° μ•μ •μ„±) |
| `learning_rate` | 0.0001 | λ‚®μ€ ν•™μµλ¥  (μ•μ „μ„±) |
| `epsilon` | 0.1 | λ‚®μ€ μ΄κΈ° νƒν—λ¥  |
| `tau` | 0.005 | λ³΄μμ  νƒ€κ² λ„¤νΈμ›ν¬ μ—…λ°μ΄νΈ |

#### **μ•΅μ… κ³µκ°„ (6κ° λ€μ‘ μμ¤€)**
```python
action_names = {
    0: 'allow',           # ν—μ© (μ •μƒ μ²λ¦¬)
    1: 'block_temporary', # μ„μ‹ μ°¨λ‹¨ (30λ¶„)
    2: 'block_permanent', # μκµ¬ μ°¨λ‹¨
    3: 'rate_limit',      # λ μ΄νΈ μ ν•
    4: 'deep_inspection', # μ¶”κ°€ κ²€μ‚¬
    5: 'isolate_session'  # μ„Έμ… κ²©λ¦¬
}
```

#### **μƒνƒ κ³µκ°„ (10μ°¨μ›)**
1. **RF νƒμ§€ κ²°κ³Ό** (4μ°¨μ›)
   - μ„ν‘ ν™•λ¥ 
   - μ‹ λΆ°λ„
   - κ³µκ²© μ ν•
   - μ‹¬κ°λ„ μμ¤€

2. **μ‹μ¤ν… μƒνƒ** (4μ°¨μ›)
   - CPU μ‚¬μ©λ¥ 
   - λ©”λ¨λ¦¬ μ‚¬μ©λ¥ 
   - ν™μ„± μ„ν‘ μ
   - μ°¨λ‹¨λ IP μ

3. **μ»¨ν…μ¤νΈ μ •λ³΄** (2μ°¨μ›)
   - ν„μ¬ μ‹κ°„ (μ‹κ°„λ€)
   - λ„¤νΈμ›ν¬ λ¶€ν•

### 2. Double DQN (κΈ°μ΅΄ μ‹μ¤ν…)

#### **ν•µμ‹¬ νΉμ§•**
```python
# Double DQN νƒ€κ² κ³„μ‚°
next_actions = self.model(next_states).argmax(1)  # ν„μ¬ λ¨λΈλ΅ μ•΅μ… μ„ νƒ
next_q_values = self.target_model(next_states).gather(1, next_actions)  # νƒ€κ² λ¨λΈλ΅ Qκ°’ κ³„μ‚°
```

#### **νλΌλ―Έν„° μ„¤μ • (μΌλ°μ  μ ‘κ·Ό)**
| νλΌλ―Έν„° | κ°’ | λ©μ  |
|----------|----|----- |
| `gamma` | 0.95 | ν‘μ¤€ ν• μΈμ¨ |
| `learning_rate` | 0.001 | ν‘μ¤€ ν•™μµλ¥  (Conservativeμ 10λ°°) |
| `epsilon` | 1.0 β†’ 0.01 | μ κ·Ήμ  νƒν— |
| `epsilon_decay` | 0.995 | λΉ λ¥Έ νƒν—λ¥  κ°μ† |

#### **μ•΅μ… κ³µκ°„ (3κ° κΈ°λ³Έ μ•΅μ…)**
```python
# λ‹¨μν• μ•΅μ… κµ¬μ΅°
0: ν—μ©
1: μ°¨λ‹¨  
2: λ¨λ‹ν„°λ§
```

---

## ν„μ¬ μ‚¬μ© κµ¬μ΅°

### π”„ **μ‹¤μ  μ‹¤ν–‰ νλ¦„**

#### **IPSAgent_RL.py (λ©”μΈ μ‹μ¤ν…)**
1. **Colab ν™κ²½**: Conservative RL μ‹μ¤ν…λ§ λ΅λ“ λ° ν•™μµ
2. **λ΅μ»¬ ν™κ²½**: Conservative RL μ‹μ¤ν…μΌλ΅ μ‹¤μ‹κ°„ λ€μ‘
3. **κΈ°μ΅΄ DQN**: μ§€μ—° λ΅λ”©μ—λ§ λ“±λ΅, μ‹¤μ  μ‚¬μ© μ•ν•¨

#### **ips_pipeline_integrator.py (ν†µν•© νμ΄ν”„λΌμΈ)**
```python
# μ‹¤μ  μ‚¬μ©λλ” μ•κ³ λ¦¬μ¦
self.rl_agent = ConservativeRLAgent(state_size=10, action_size=6)

# μ‹¤μ‹κ°„ λ€μ‘ κ²°μ •
action_id = self.rl_agent.act(rl_state, deterministic=True)
```

### π“ **μ„±λ¥ λ° μ‚¬μ©λ¥ **

| κµ¬λ¶„ | Conservative RL | κΈ°μ΅΄ DQN |
|------|----------------|----------|
| **μ‹¤μ  μ‚¬μ©λ¥ ** | 100% (λ©”μΈ) | 0% (λΉ„ν™μ„±ν™”) |
| **λ©”λ¨λ¦¬ μ‚¬μ©** | ~160MB | 0MB (λ΅λ“ μ•λ¨) |
| **μ²λ¦¬ μ†λ„** | μ•μ •μ  | N/A |
| **μ •ν™•λ„** | OPE κ²€μ¦λ¨ | ν…μ¤νΈμ©λ§ |

---

## μ¥λ‹¨μ  λΉ„κµ λ¶„μ„

### β… **Conservative Q-Learning μ¥μ  (ν„μ¬ μ‚¬μ©)**

#### **1. IPS μ‹μ¤ν… νΉν™” μ•μ „μ„±**
- **κ³Όλ€μ¶”μ • λ°©μ§€**: Conservative νλ„ν‹°λ΅ μ„ν—ν• μ•΅μ… μ–µμ 
- **λ³΄μμ  νƒν—**: λ‚®μ€ epsilon (0.1)μΌλ΅ μμΈ΅ κ°€λ¥ν• ν–‰λ™
- **μ•μ „ν• ν•™μµ**: λ‚®μ€ ν•™μµλ¥ λ΅ κΈ‰κ²©ν• μ •μ±… λ³€ν™” λ°©μ§€

#### **2. μ •κµν• λ€μ‘ μ „λµ**
- **6λ‹¨κ³„ λ€μ‘**: ν—μ© β†’ μ„μ‹μ°¨λ‹¨ β†’ μκµ¬μ°¨λ‹¨ β†’ λ μ΄νΈμ ν• β†’ μ¶”κ°€κ²€μ‚¬ β†’ κ²©λ¦¬
- **λΉ„μ© κΈ°λ° λ³΄μƒ**: μ‹¤μ  λΉ„μ¦λ‹μ¤ μν–¥ κ³ λ ¤
```python
# μ„ν‘ μμ¤€λ³„ μ„Έλ°€ν• λ³΄μƒ μ„¤κ³„
if threat_level > 0.9:  # ν™•μ‹¤ν• κ³µκ²©
    if action in [1, 2]:  # κ°•ν• λ€μ‘
        base_reward += 15.0
elif threat_level > 0.7:  # μμ‹¬μ¤λ¬μ΄
    if action in [1, 3, 4]:  # μ μ ν• λ€μ‘
        base_reward += 10.0
```

#### **3. μΆ…ν•©μ  μƒνƒ λ¨λΈλ§**
- **10μ°¨μ› μƒνƒ**: RF κ²°κ³Ό + μ‹μ¤ν… μƒνƒ + μ»¨ν…μ¤νΈ
- **KISTI RF μ—°λ™**: ν„μ‹¤μ  μ„ν‘ νƒμ§€ κ²°κ³Ό ν™μ©
- **μ‹¤μ‹κ°„ μ‹μ¤ν… μ •λ³΄**: CPU, λ©”λ¨λ¦¬, λ„¤νΈμ›ν¬ λ¶€ν• λ°μ

#### **4. OPE κΈ°λ° κ²€μ¦**
- **4κ°€μ§€ ν‰κ°€ λ°©λ²•**: Importance Sampling, Doubly Robust, WIS, Direct Method
- **ν†µκ³„μ  κ²€μ •**: μ‹ λΆ°κµ¬κ°„, μ μμ„± κ²€μ •μΌλ΅ κ°κ΄€μ  μ„±λ¥ ν‰κ°€

### β **Conservative Q-Learning λ‹¨μ **

#### **1. ν•™μµ ν¨μ¨μ„±**
- **λλ¦° μλ ΄**: λ³΄μμ  νλΌλ―Έν„°λ΅ ν•™μµ μ‹κ°„ μ¦κ°€
- **μ ν•λ νƒν—**: μƒλ΅μ΄ μ „λµ λ°κ²¬ μ–΄λ ¤μ›€
- **λ³µμ΅ν• κ³„μ‚°**: Conservative νλ„ν‹° κ³„μ‚° μ¤λ²„ν—¤λ“

#### **2. κ³„μ‚° λ³µμ΅λ„**
- **λ†’μ€ λ©”λ¨λ¦¬**: 10μ°¨μ› μƒνƒ + 6κ° μ•΅μ…
- **OPE μ¤λ²„ν—¤λ“**: λ‹¤μ¤‘ ν‰κ°€ λ°©λ²•μΌλ΅ κ³„μ‚° λ¶€λ‹΄

### β… **κΈ°μ΅΄ DQN μ¥μ  (μ°Έκ³ μ©)**

#### **1. ν•™μµ ν¨μ¨μ„±**
- **λΉ λ¥Έ μλ ΄**: λ†’μ€ ν•™μµλ¥  (0.001)κ³Ό μ κ·Ήμ  νƒν—
- **λ‹¨μν• κµ¬μ΅°**: 3κ° μ•΅μ…μΌλ΅ λΉ λ¥Έ κ²°μ •
- **ν‘μ¤€ μ•κ³ λ¦¬μ¦**: κ²€μ¦λ Double DQN κµ¬ν„

#### **2. λ²”μ©μ„±**
- **μ μ—°ν• ν™κ²½**: NetworkEnvμ—μ„ μ§μ ‘ ν¨ν‚· μ²λ¦¬
- **λ¨λ“ μ§€μ›**: lightweight(7νΉμ„±) / performance(12νΉμ„±)

### β **κΈ°μ΅΄ DQN λ‹¨μ  (μ‚¬μ© μ•ν•λ” μ΄μ )**

#### **1. λ³΄μ• μ‹μ¤ν… λ¶€μ ν•©**
- **κ³Όλ€μ¶”μ • μ„ν—**: Qκ°’ κ³Όλ€μ¶”μ •μΌλ΅ μ„ν—ν• μ•΅μ… μ„ νƒ κ°€λ¥
- **κ³µκ²©μ  νƒν—**: λ†’μ€ epsilonμΌλ΅ μμΈ΅ λ¶κ°€λ¥ν• ν–‰λ™
- **λ‹¨μν• μ•΅μ…**: 3κ° μ•΅μ…μΌλ΅ μ„Έλ°€ν• λ€μ‘ λ¶κ°€

#### **2. μ•μ „μ„± λ¶€μ΅±**
- **μ¤νƒ μ„ν—**: μ •μƒ νΈλν”½ μ°¨λ‹¨μΌλ΅ μ„λΉ„μ¤ μ¤‘λ‹¨ κ°€λ¥
- **κ²€μ¦ λ¶€μ΅±**: μ¤ν”„λΌμΈ ν‰κ°€ μ‹μ¤ν… μ—†μ

---

## κ°μ„  λ°©ν–¥ λ° κ¶μ¥μ‚¬ν•­

###  **λ‹¨κΈ° κ°μ„  (1-2μ£Ό)**

#### **1. Conservative RL νλΌλ―Έν„° νλ‹**
```python
# ν„μ¬ νλΌλ―Έν„° (λ§¤μ° λ³΄μμ )
alpha_cql = 1.0        # β†’ 0.5-0.8λ΅ μ΅°μ • κ²€ν† 
learning_rate = 0.0001 # β†’ 0.0005λ΅ μ¦κ°€ κ²€ν† 
epsilon = 0.1          # β†’ 0.15λ΅ μ¦κ°€ κ²€ν† 

# λ©μ : μ•μ „μ„± μ μ§€ν•λ©΄μ„ ν•™μµ ν¨μ¨μ„± ν–¥μƒ
```

#### **2. κΈ°μ΅΄ DQN μ½”λ“ μ •λ¦¬**
```python
# κ°μ„  μ‘μ—…
1. μ‚¬μ©ν•μ§€ μ•λ” DQN κ΄€λ ¨ μ½”λ“ μ£Όμ„ μ²λ¦¬
2. μ§€μ—° λ΅λ”©μ—μ„ DQN μ κ±° (λ©”λ¨λ¦¬ μ μ•½)
3. NetworkEnv ν΄λμ¤ λ‹¨μν™” (ν…μ¤νΈμ©μΌλ΅λ§ μ μ§€)
4. νΈν™μ„± μ μ§€λ¥Ό μ„ν• μµμ†ν•μ μΈν„°νμ΄μ¤λ§ λ³΄μ΅΄
```

#### **3. OPE ν‰κ°€ κ°•ν™”**
```python
# ν‰κ°€ μ£ΌκΈ° μµμ ν™”
- μ‹¤μ‹κ°„ μ •μ±… μ„±λ¥ λ¨λ‹ν„°λ§ κ°•ν™”
- μ΄μƒ μ •μ±… κ°μ§€ μ‹ μλ™ μ•λ¦Ό
- μ •μ±… μ„±λ¥ μ €ν• μ‹ μλ™ μ¬ν•™μµ νΈλ¦¬κ±°
```

### **μ¤‘κΈ° κ°μ„  (1-2κ°μ›”)**

#### **1. μ μ‘ν• Conservative νλΌλ―Έν„°**
```python
class AdaptiveConservativeRL:
    def __init__(self):
        # μ‹μ¤ν… λ¶€ν•μ— λ”°λ¥Έ λ™μ  νλΌλ―Έν„° μ΅°μ 
        self.base_alpha_cql = 1.0
        self.adaptive_alpha = True
        
    def adjust_conservativeness(self, system_load, threat_level):
        """μ‹μ¤ν… μƒν™©μ— λ”°λ¥Έ λ³΄μμ„± μ΅°μ """
        if system_load > 0.8:  # λ†’μ€ λ¶€ν•
            self.alpha_cql = self.base_alpha_cql * 1.5  # λ” λ³΄μμ 
        elif threat_level < 0.3:  # λ‚®μ€ μ„ν—
            self.alpha_cql = self.base_alpha_cql * 0.7  # λ λ³΄μμ 
        else:
            self.alpha_cql = self.base_alpha_cql  # κΈ°λ³Έκ°’
```

#### **2. ν•μ΄λΈλ¦¬λ“ λ³΄μƒ ν•¨μ**
```python
def calculate_hybrid_reward(self, action, threat_result, system_state):
    """λ‹¤μ¤‘ λ©μ  μµμ ν™” λ³΄μƒ ν•¨μ"""
    
    # 1. λ³΄μ• ν¨κ³Όμ„± (κ°€μ¥ μ¤‘μ”)
    security_reward = self._calculate_security_reward(action, threat_result)
    
    # 2. μ‹μ¤ν… ν¨μ¨μ„±
    efficiency_reward = self._calculate_efficiency_reward(action, system_state)
    
    # 3. λΉ„μ¦λ‹μ¤ μν–¥
    business_reward = self._calculate_business_reward(action, threat_result)
    
    # κ°€μ¤‘ ν•©μ‚°
    total_reward = (
        0.6 * security_reward +    # λ³΄μ• 60%
        0.25 * efficiency_reward + # ν¨μ¨μ„± 25%
        0.15 * business_reward     # λΉ„μ¦λ‹μ¤ 15%
    )
    
    return total_reward
```

#### **3. λ©”νƒ€ ν•™μµ μ‹μ¤ν…**
```python
class MetaLearningRL:
    """μƒν™©λ³„ μµμ  νλΌλ―Έν„° μλ™ ν•™μµ"""
    
    def __init__(self):
        self.situation_history = []
        self.performance_history = []
        
    def adapt_to_situation(self, current_situation):
        """ν„μ¬ μƒν™©μ— μµμ ν™”λ νλΌλ―Έν„° μ¶”μ²"""
        similar_situations = self._find_similar_situations(current_situation)
        optimal_params = self._analyze_best_parameters(similar_situations)
        return optimal_params
```

###  **μ¥κΈ° κ°μ„ **

#### **1. κ³ κΈ‰ κ°•ν™”ν•™μµ μ•κ³ λ¦¬μ¦ λ„μ…**

**Proximal Policy Optimization (PPO)**
```python
class PPOSecurityAgent:
    """λ³΄μ• μ‹μ¤ν… νΉν™” PPO μ—μ΄μ „νΈ"""
    
    def __init__(self):
        # PPO νΉν™” νλΌλ―Έν„°
        self.clip_epsilon = 0.1      # λ‚®μ€ ν΄λ¦¬ν•‘ (λ³΄μμ )
        self.entropy_coeff = 0.001   # λ‚®μ€ μ—”νΈλ΅ν”Ό (μ•μ •μ„±)
        self.value_loss_coeff = 0.5
        
    def calculate_ppo_loss(self, states, actions, rewards, old_probs):
        """λ³΄μ• μ‹μ¤ν… νΉν™” PPO μ†μ‹¤"""
        # κΈ°λ³Έ PPO μ†μ‹¤ + λ³΄μ• μ μ•½ μ΅°κ±΄
        pass
```

**Soft Actor-Critic (SAC)**
```python
class SACSecurityAgent:
    """μ—°μ† μ•΅μ… κ³µκ°„μ„ μ„ν• SAC μ—μ΄μ „νΈ"""
    
    def __init__(self):
        # μ—°μ† λ€μ‘ κ°•λ„ μ΅°μ  (0.0 ~ 1.0)
        self.action_space = "continuous"  # μ°¨λ‹¨ κ°•λ„, λ μ΄νΈ μ ν• μ •λ„ λ“±
```

#### **2. λ©€ν‹° μ—μ΄μ „νΈ μ‹μ¤ν…**
```python
class MultiAgentIPS:
    """λ‹¤μ¤‘ μ—μ΄μ „νΈ ν‘λ ¥ μ‹μ¤ν…"""
    
    def __init__(self):
        self.detection_agent = ConservativeRLAgent()  # νƒμ§€ μ „λ‹΄
        self.response_agent = ConservativeRLAgent()   # λ€μ‘ μ „λ‹΄
        self.coordination_agent = ConservativeRLAgent()  # μ΅°μ • μ „λ‹΄
        
    def coordinate_response(self, threat_info):
        """μ—μ΄μ „νΈ κ°„ ν‘λ ¥μ  λ€μ‘"""
        detection_result = self.detection_agent.act(threat_info)
        response_plan = self.response_agent.act(detection_result)
        final_action = self.coordination_agent.act([detection_result, response_plan])
        return final_action
```

#### **3. μ„¤λ… κ°€λ¥ν• AI (XAI) ν†µν•©**
```python
class ExplainableRL:
    """μ„¤λ… κ°€λ¥ν• κ°•ν™”ν•™μµ μ‹μ¤ν…"""
    
    def explain_decision(self, state, action):
        """RL κ²°μ •μ— λ€ν• μ„¤λ… μƒμ„±"""
        explanation = {
            'action': action,
            'confidence': self._calculate_confidence(state, action),
            'key_factors': self._identify_key_factors(state),
            'alternative_actions': self._suggest_alternatives(state),
            'risk_assessment': self._assess_risks(state, action)
        }
        return explanation
```

---

## κµ¬ν„ μ°μ„ μμ„

###  **μ°μ„ μμ„ 1: ν„μ¬ μ‹μ¤ν… μµμ ν™” (μ¦‰μ‹ μ‹μ‘)**

#### **A. νλΌλ―Έν„° νλ‹ μ‹¤ν—**
```bash
# μ‹¤ν— κ³„ν
1. alpha_cql μ΅°μ •: 1.0 β†’ 0.5, 0.7, 0.8 ν…μ¤νΈ
2. learning_rate μ΅°μ •: 0.0001 β†’ 0.0002, 0.0005 ν…μ¤νΈ  
3. epsilon μ΅°μ •: 0.1 β†’ 0.12, 0.15 ν…μ¤νΈ
4. κ° μ΅°ν•©λ³„ OPE μ„±λ¥ ν‰κ°€ λ° λΉ„κµ
```

#### **B. μ½”λ“ μ •λ¦¬ λ° μµμ ν™”**
```bash
# μ •λ¦¬ μ‘μ—…
1. κΈ°μ΅΄ DQN κ΄€λ ¨ μ½”λ“ μ£Όμ„ μ²λ¦¬
2. μ§€μ—° λ΅λ”©μ—μ„ λ¶ν•„μ”ν• λ¨λ“ μ κ±°
3. Conservative RL μ „μ© μµμ ν™”
4. λ©”λ¨λ¦¬ μ‚¬μ©λ‰ μ¶”κ°€ μ μ•½ (μμƒ: 20-30MB)
```

###  **μ°μ„ μμ„ 2: μ μ‘ν• μ‹μ¤ν… κµ¬μ¶• (1κ°μ›” ν›„)**

#### **A. λ™μ  νλΌλ―Έν„° μ΅°μ **
- μ‹μ¤ν… λ¶€ν•, μ„ν‘ μμ¤€μ— λ”°λ¥Έ μλ™ νλΌλ―Έν„° μ΅°μ 
- μ‹¤μ‹κ°„ μ„±λ¥ λ¨λ‹ν„°λ§ κΈ°λ° μ μ‘

#### **B. ν–¥μƒλ λ³΄μƒ ν•¨μ**
- λ‹¤μ¤‘ λ©μ  μµμ ν™” (λ³΄μ•, ν¨μ¨μ„±, λΉ„μ¦λ‹μ¤)
- λ„λ©”μΈ μ „λ¬Έκ°€ μ§€μ‹ ν†µν•©

###  **μ°μ„ μμ„ 3: κ³ κΈ‰ μ•κ³ λ¦¬μ¦ λ„μ… (3κ°μ›” ν›„)**

#### **A. PPO λλ” SAC λ„μ…**
- Conservative Q-Learning λ€λΉ„ μ„±λ¥ λΉ„κµ
- μ—°μ† μ•΅μ… κ³µκ°„ μ§€μ› κ²€ν† 

#### **B. λ©€ν‹° μ—μ΄μ „νΈ μ‹μ¤ν…**
- νƒμ§€-λ€μ‘-μ΅°μ • μ—­ν•  λ¶„λ¦¬
- μ—μ΄μ „νΈ κ°„ ν‘λ ¥ λ©”μ»¤λ‹μ¦

---

## μ‹¤ν— λ° κ²€μ¦ κ³„ν

###  **μ„±λ¥ λΉ„κµ λ©”νΈλ¦­**

#### **1. λ³΄μ• μ„±λ¥**
- **νƒμ§€μ¨**: True Positive Rate
- **μ¤νƒλ¥ **: False Positive Rate  
- **λ€μ‘ μ μ μ„±**: μ„ν‘ μμ¤€ λ€λΉ„ λ€μ‘ κ°•λ„ μΌμΉλ„
- **μ°¨λ‹¨ ν¨κ³Όμ„±**: μ‹¤μ  κ³µκ²© μ°¨λ‹¨ μ„±κ³µλ¥ 

#### **2. μ‹μ¤ν… μ„±λ¥**
- **μ‘λ‹µ μ‹κ°„**: νƒμ§€λ¶€ν„° λ€μ‘κΉμ§€ μ†μ” μ‹κ°„
- **μ²λ¦¬λ‰**: μ΄λ‹Ή μ²λ¦¬ κ°€λ¥ν• ν¨ν‚· μ
- **λ©”λ¨λ¦¬ μ‚¬μ©λ‰**: μ•κ³ λ¦¬μ¦λ³„ λ©”λ¨λ¦¬ μ†λΉ„
- **CPU μ‚¬μ©λ¥ **: μ•κ³ λ¦¬μ¦ μ‹¤ν–‰ μ‹ CPU λ¶€ν•

#### **3. μ΄μ μ•μ •μ„±**
- **μ •μ±… μΌκ΄€μ„±**: λ™μΌ μƒν™©μ—μ„ λ™μΌ μ•΅μ… μ„ νƒλ¥ 
- **ν•™μµ μ•μ •μ„±**: ν•™μµ κ³Όμ •μ—μ„ μ„±λ¥ λ³€λ™μ„±
- **μ‹μ¤ν… μν–¥**: μ„λΉ„μ¤ μ¤‘λ‹¨ λλ” μ§€μ—° λ°μƒλ¥ 

###  **μ‹¤ν— ν™κ²½ μ„¤μ •**

#### **A. ν…μ¤νΈ λ°μ΄ν„°μ…‹**
```python
# μ‹¤ν—μ© λ°μ΄ν„°
1. KISTI-IDS-2022 ν…μ¤νΈ μ…‹ (50λ§ μƒν”)
2. μ‹¤μ‹κ°„ μΊ΅μ² λ°μ΄ν„° (captured_packets_*.csv)
3. μ‹λ®¬λ μ΄μ… κ³µκ²© λ°μ΄ν„° (TrafficGeneratorApp.py)
```

#### **B. ν‰κ°€ ν™κ²½**
```python
# ν‰κ°€ μ‹λ‚λ¦¬μ¤
1. μ •μƒ νΈλν”½λ§ (μ¤νƒλ¥  μΈ΅μ •)
2. νΌν•© νΈλν”½ (μ‹¤μ  ν™κ²½ μ‹λ®¬λ μ΄μ…)  
3. κ³ κ°•λ„ κ³µκ²© (κ·Ήν• μƒν™© ν…μ¤νΈ)
4. μ‹μ¤ν… λ¶€ν• μƒν™© (λ¦¬μ†μ¤ μ μ•½ ν™κ²½)
```

---

## κ²°λ΅  λ° λ‹¤μ λ‹¨κ³„

###  **ν„μ¬ μƒν™© μ”μ•½**

1. **ν„μ¬**: Conservative Q-Learning λ‹¨λ… μ‚¬μ© β…
2. **κΈ°μ΅΄ DQN**: νΈν™μ„± μ μ§€μ© (λΉ„ν™μ„±ν™” μƒνƒ)
3. **μ„±λ¥**: μ•μ •μ μ΄μ§€λ§ κ°μ„  μ—¬μ§€ μμ
4. **κµ¬μ΅°**: λ‹¨μΌ μ•κ³ λ¦¬μ¦μΌλ΅ λ…ν™•ν•κ³  μ•μ „ν•¨

###  **μ¦‰μ‹ μ‹¤ν–‰ κ°€λ¥ν• κ°μ„ **

#### **1μ£Όμ°¨ μ‘μ—…**
- [ ] Conservative RL νλΌλ―Έν„° νλ‹ μ‹¤ν—
- [ ] κΈ°μ΅΄ DQN μ½”λ“ μ •λ¦¬ λ° μ£Όμ„ μ²λ¦¬
- [ ] OPE ν‰κ°€ μ£ΌκΈ° μµμ ν™”

#### **2μ£Όμ°¨ μ‘μ—…**  
- [ ] μµμ  νλΌλ―Έν„° μ΅°ν•© μ„ μ •
- [ ] μ„±λ¥ λ²¤μΉλ§ν¬ μν–‰
- [ ] λ©”λ¨λ¦¬ μ‚¬μ©λ‰ μ¶”κ°€ μµμ ν™”

###  **μ¥κΈ° λΉ„μ „**

- **μλ™ νλ‹**: ν™κ²½μ— λ”°λ¥Έ μλ™ νλΌλ―Έν„° μµμ ν™”
- **μ„¤λ… κ°€λ¥μ„±**: λ¨λ“  λ€μ‘ κ²°μ •μ— λ€ν• λ…ν™•ν• κ·Όκ±° μ μ‹
- **μμΈ΅ κΈ°λ¥**: κ³µκ²© ν¨ν„΄ μμΈ΅ λ° μ„ μ μ  λ€μ‘
- **λ©€ν‹° λ„λ©”μΈ**: λ„¤νΈμ›ν¬, μ›Ή, IoT λ“± λ‹¤μ–‘ν• ν™κ²½ μ§€μ›

---

**π“ μ°Έκ³  νμΌ**:
- `IDS/modules/conservative_rl_agent.py` - λ©”μΈ μ•κ³ λ¦¬μ¦
- `IDS/modules/reinforcement_learning.py` - κΈ°μ΅΄ μ‹μ¤ν… (μ°Έκ³ μ©)
- `IDS/modules/defense_policy_env.py` - RL ν™κ²½
- `IDS/modules/ope_evaluator.py` - μ„±λ¥ ν‰κ°€
- `IDS/ips_pipeline_integrator.py` - ν†µν•© νμ΄ν”„λΌμΈ
